{
    "docs": [
        {
            "location": "/", 
            "text": "Introduction\n\n\nSematch is an integrated framework for the development, evaluation, and application of semantic similarity for Knowledge Graphs (KGs). It is easy to use Sematch to compute semantic similarity scores of concepts, words and entities. Sematch focuses on specific knowledge-based semantic similarity metrics that rely on structural knowledge in taxonomy (e.g. depth, path length, least common subsumer), and statistical information contents (corpus-IC and graph-IC). Knowledge-based approaches are different from their conterpart corpus-based approaches, which are mainly based on word coocurrence (e.g. Pointwise Mutual Information) or distributional semantics (Latent Semantic Analysis, Word2Vec, GLOVE and etc). Knowledge-based approaches are usually used for structual KGs, while corpus-based approaches are normally applied in textual corpora.\n\n\nIn text analysis applications, a common pipeline is adolpted in using semantic similarity from concept level, to word and sentence level. For example, word similarity is first computed based on similarity scores of WordNet concepts, and sentence similarity is computed by composing word similarity scores. Finally, document similarity could be computed by identifying important sentences, e.g. TextRank.  \n\n\n\n\nKG based applications also meet similar pipeline in using semantic similarity, from concept similarity (e.g. \nhttp://dbpedia.org/class/yago/Actor109765278\n) to entity similarity (e.g. \nhttp://dbpedia.org/resource/Madrid\n). Furthermore, in computing document similarity, entities are extracted and document similarity is computed by composing entity similarity scores. \n\n\n\n\nIn KGs, concepts usually denote ontology classes while entities refer to ontology instances. Moreover, those concepts are usually constructed into hierarchical taxonomies, such as DBpedia ontology class, thus quantifying concept similarity in KG relies on similar semantic information (e.g. path length,  depth, least common subsumer, information content) and semantic similarity metrics (e.g. Path, Wu \n Palmer,Li,  Resnik, Lin, Jiang \n Conrad and WPath). In consequence,  Sematch provides an integrated framework to develop and evaluate semantic similarity metrics for concepts, words, entities and their applications. \n\n\n\n\nGetting started: 20 minutes to Sematch\n\n\nInstall Sematch\n\n\nYou need to install scientific computing libraries \nnumpy\n and \nscipy\n first. An example of installing them with pip is shown below.\n\n\npip install numpy scipy\n\n\n\n\nDepending on different OS, you can use different ways to install them. After sucessful installation of \nnumpy\n and \nscipy\n, you can install sematch with following commands.\n\n\npip install sematch\npython -m sematch.download\n\n\n\n\nAlternatively, you can use the development version to clone and install Sematch with setuptools. We recommend you to update your pip and setuptools.\n\n\ngit clone https://github.com/gsi-upm/sematch.git\ncd sematch\npython setup.py install\n\n\n\n\nWe also provide a \nSematch-Demo Server\n. You can use it for experimenting with main functionalities or take it as an example for using Sematch to develop applications. Please check our \nDocumentation\n for more details.\n\n\nComputing Word Similarity\n.\n\n\nThe core module of Sematch is measuring semantic similarity between concepts that are represented as concept taxonomies. Word similarity is computed based on the maximum semantic similarity of WordNet concepts. You can use Sematch to compute multilingual word similarity based on WordNet with various of semantic similarity metrics.\n\n\nfrom sematch.semantic.similarity import WordNetSimilarity\nwns = WordNetSimilarity()\n\n# Computing English word similarity using Li method\nwns.word_similarity('dog', 'cat', 'li') # 0.449327301063\n# Computing Spanish word similarity using Lin method\nwns.monol_word_similarity('perro', 'gato', 'spa', 'lin') #0.876800984373\n# Computing Chinese word similarity using  Wu \n Palmer method\nwns.monol_word_similarity('\u72d7', '\u732b', 'cmn', 'wup') # 0.857142857143\n# Computing Spanish and English word similarity using Resnik method\nwns.crossl_word_similarity('perro', 'cat', 'spa', 'eng', 'res') #7.91166650904\n# Computing Spanish and Chinese word similarity using Jiang \n Conrad method\nwns.crossl_word_similarity('perro', '\u732b', 'spa', 'cmn', 'jcn') #0.31023804699\n# Computing Chinese and English word similarity using WPath method\nwns.crossl_word_similarity('\u72d7', 'cat', 'cmn', 'eng', 'wpath')#0.593666388463\n\n\n\n\nComputing semantic similarity of YAGO concepts\n.\n\n\nfrom sematch.semantic.similarity import YagoTypeSimilarity\nsim = YagoTypeSimilarity()\n\n#Measuring YAGO concept similarity through WordNet taxonomy and corpus based information content\nsim.yago_similarity('http://dbpedia.org/class/yago/Dancer109989502','http://dbpedia.org/class/yago/Actor109765278', 'wpath') #0.642\nsim.yago_similarity('http://dbpedia.org/class/yago/Dancer109989502','http://dbpedia.org/class/yago/Singer110599806', 'wpath') #0.544\n#Measuring YAGO concept similarity based on graph-based IC\nsim.yago_similarity('http://dbpedia.org/class/yago/Dancer109989502','http://dbpedia.org/class/yago/Actor109765278', 'wpath_graph') #0.423\nsim.yago_similarity('http://dbpedia.org/class/yago/Dancer109989502','http://dbpedia.org/class/yago/Singer110599806', 'wpath_graph') #0.328\n\n\n\n\nComputing semantic similarity of DBpedia concepts\n.\n\n\nfrom sematch.semantic.graph import DBpediaDataTransform, Taxonomy\nfrom sematch.semantic.similarity import ConceptSimilarity\nconcept = ConceptSimilarity(Taxonomy(DBpediaDataTransform()),'models/dbpedia_type_ic.txt')\nconcept.name2concept('actor')\nconcept.similarity('http://dbpedia.org/ontology/Actor','http://dbpedia.org/ontology/Film', 'path')\nconcept.similarity('http://dbpedia.org/ontology/Actor','http://dbpedia.org/ontology/Film', 'wup')\nconcept.similarity('http://dbpedia.org/ontology/Actor','http://dbpedia.org/ontology/Film', 'li')\nconcept.similarity('http://dbpedia.org/ontology/Actor','http://dbpedia.org/ontology/Film', 'res')\nconcept.similarity('http://dbpedia.org/ontology/Actor','http://dbpedia.org/ontology/Film', 'lin')\nconcept.similarity('http://dbpedia.org/ontology/Actor','http://dbpedia.org/ontology/Film', 'jcn')\nconcept.similarity('http://dbpedia.org/ontology/Actor','http://dbpedia.org/ontology/Film', 'wpath')\n\n\n\n\nComputing semantic similarity of DBpedia entities\n.\n\n\nfrom sematch.semantic.similarity import EntitySimilarity\nsim = EntitySimilarity()\nsim.similarity('http://dbpedia.org/resource/Madrid','http://dbpedia.org/resource/Barcelona') #0.409923677282\nsim.similarity('http://dbpedia.org/resource/Apple_Inc.','http://dbpedia.org/resource/Steve_Jobs')#0.0904545454545\nsim.relatedness('http://dbpedia.org/resource/Madrid','http://dbpedia.org/resource/Barcelona')#0.457984139871\nsim.relatedness('http://dbpedia.org/resource/Apple_Inc.','http://dbpedia.org/resource/Steve_Jobs')#0.465991132787\n\n\n\n\nEvaluate semantic similarity metrics with word similarity datasets\n.\n\n\nfrom sematch.evaluation import WordSimEvaluation\nfrom sematch.semantic.similarity import WordNetSimilarity\nevaluation = WordSimEvaluation()\nevaluation.dataset_names()\nwns = WordNetSimilarity()\n# define similarity metrics\nwpath = lambda x, y: wns.word_similarity_wpath(x, y, 0.8)\n# evaluate similarity metrics with SimLex dataset\nevaluation.evaluate_metric('wpath', wpath, 'noun_simlex')\n# performa Steiger's Z significance Test\nevaluation.statistical_test('wpath', 'path', 'noun_simlex')\n# define similarity metrics for Spanish words\nwpath_es = lambda x, y: wns.monol_word_similarity(x, y, 'spa', 'path')\n# define cross-lingual similarity metrics for English-Spanish\nwpath_en_es = lambda x, y: wns.crossl_word_similarity(x, y, 'eng', 'spa', 'wpath')\n# evaluate metrics in multilingual word similarity datasets\nevaluation.evaluate_metric('wpath_es', wpath_es, 'rg65_spanish')\nevaluation.evaluate_metric('wpath_en_es', wpath_en_es, 'rg65_EN-ES')\n\n\n\n\nEvaluate semantic similarity metrics with category classification\n.\n\n\nAlthough the word similarity correlation measure is the standard way to evaluate the semantic similarity metrics, it relies on human judgements over word pairs which may not have same performance in real applications. Therefore, apart from word similarity evaluation, the Sematch evaluation framework also includes a simple aspect category classification. The task classifies noun concepts such as pasta, noodle, steak, tea into their ontological parent concept FOOD, DRINKS.\n\n\nfrom sematch.evaluation import AspectEvaluation\nfrom sematch.application import SimClassifier, SimSVMClassifier\nfrom sematch.semantic.similarity import WordNetSimilarity\n\n# create aspect classification evaluation\nevaluation = AspectEvaluation()\n# load the dataset\nX, y = evaluation.load_dataset()\n# define word similarity function\nwns = WordNetSimilarity()\nword_sim = lambda x, y: wns.word_similarity(x, y)\n# Train and evaluate metrics with unsupervised classification model\nsimclassifier = SimClassifier.train(zip(X,y), word_sim)\nevaluation.evaluate(X,y, simclassifier)\n\nmacro averge:  (0.65319812882333839, 0.7101245049198579, 0.66317566364913016, None)\nmicro average:  (0.79210167952791644, 0.79210167952791644, 0.79210167952791644, None)\nweighted average:  (0.80842645056024054, 0.79210167952791644, 0.79639496616636352, None)\naccuracy:  0.792101679528\n             precision    recall  f1-score   support\n\n    SERVICE       0.50      0.43      0.46       519\n RESTAURANT       0.81      0.66      0.73       228\n       FOOD       0.95      0.87      0.91      2256\n   LOCATION       0.26      0.67      0.37        54\n   AMBIENCE       0.60      0.70      0.65       597\n     DRINKS       0.81      0.93      0.87       752\n\navg / total       0.81      0.79      0.80      4406\n\n\n\n\nYou can use Sematch to download a list of entities having a specific type using different languages. Sematch will generate SPARQL queries and execute them in \nDBpedia Sparql Endpoint\n.\n\n\nfrom sematch.application import Matcher\nmatcher = Matcher()\n# matching scientist entities from DBpedia\nmatcher.match_type('scientist')\nmatcher.match_type('cient\u00edfico', 'spa')\nmatcher.match_type('\u79d1\u5b66\u5bb6', 'cmn')\n\n\n----SPARQL queries----\nSELECT DISTINCT ?s, ?label, ?abstract WHERE {\n    {  \n    ?s \nhttp://www.w3.org/1999/02/22-rdf-syntax-ns#type\n \nhttp://dbpedia.org/class/yago/NuclearPhysicist110364643\n . }\n UNION {  \n    ?s \nhttp://www.w3.org/1999/02/22-rdf-syntax-ns#type\n \nhttp://dbpedia.org/class/yago/Econometrician110043491\n . }\n UNION {  \n    ?s \nhttp://www.w3.org/1999/02/22-rdf-syntax-ns#type\n \nhttp://dbpedia.org/class/yago/Sociologist110620758\n . }\n UNION {  \n    ?s \nhttp://www.w3.org/1999/02/22-rdf-syntax-ns#type\n \nhttp://dbpedia.org/class/yago/Archeologist109804806\n . }\n UNION {  \n    ?s \nhttp://www.w3.org/1999/02/22-rdf-syntax-ns#type\n \nhttp://dbpedia.org/class/yago/Neurolinguist110354053\n . } \n    ?s \nhttp://www.w3.org/1999/02/22-rdf-syntax-ns#type\n \nhttp://www.w3.org/2002/07/owl#Thing\n . \n    ?s \nhttp://www.w3.org/2000/01/rdf-schema#label\n ?label . \n    FILTER( lang(?label) = \nen\n) . \n    ?s \nhttp://dbpedia.org/ontology/abstract\n ?abstract . \n    FILTER( lang(?abstract) = \nen\n) .\n} LIMIT 5000\n\n\n\n\nMatching Entities with type using SPARQL queries\n.\n\n\nYou can use Sematch to download a list of entities having a specific type using different languages. Sematch will generate SPARQL queries and execute them in \nDBpedia Sparql Endpoint\n.\n\n\nfrom sematch.application import Matcher\nmatcher = Matcher()\n# matching scientist entities from DBpedia\nmatcher.match_type('scientist')\nmatcher.match_type('cient\u00edfico', 'spa')\nmatcher.match_type('\u79d1\u5b66\u5bb6', 'cmn')\nmatcher.match_entity_type('movies with Tom Cruise')\n\n\n\n\nExample of automatically generated SPARQL query.\n\n\nSELECT DISTINCT ?s, ?label, ?abstract WHERE {\n    {  \n    ?s \nhttp://www.w3.org/1999/02/22-rdf-syntax-ns#type\n \nhttp://dbpedia.org/class/yago/NuclearPhysicist110364643\n . }\n UNION {  \n    ?s \nhttp://www.w3.org/1999/02/22-rdf-syntax-ns#type\n \nhttp://dbpedia.org/class/yago/Econometrician110043491\n . }\n UNION {  \n    ?s \nhttp://www.w3.org/1999/02/22-rdf-syntax-ns#type\n \nhttp://dbpedia.org/class/yago/Sociologist110620758\n . }\n UNION {  \n    ?s \nhttp://www.w3.org/1999/02/22-rdf-syntax-ns#type\n \nhttp://dbpedia.org/class/yago/Archeologist109804806\n . }\n UNION {  \n    ?s \nhttp://www.w3.org/1999/02/22-rdf-syntax-ns#type\n \nhttp://dbpedia.org/class/yago/Neurolinguist110354053\n . } \n    ?s \nhttp://www.w3.org/1999/02/22-rdf-syntax-ns#type\n \nhttp://www.w3.org/2002/07/owl#Thing\n . \n    ?s \nhttp://www.w3.org/2000/01/rdf-schema#label\n ?label . \n    FILTER( lang(?label) = \nen\n) . \n    ?s \nhttp://dbpedia.org/ontology/abstract\n ?abstract . \n    FILTER( lang(?abstract) = \nen\n) .\n} LIMIT 5000\n\n\n\n\nEntity feature extraction with Similarity Graph\n\n\nApart from semantic matching of entities from DBpedia, you can also use Sematch to extract features of entities and apply semantic similarity analysis using graph-based ranking algorithms. Given a list of objects (concepts, words, entities), Sematch compute their pairwise semantic similarity and generate similarity graph where nodes denote objects and edges denote similarity scores. An example of using similarity graph for extracting important words from an entity description.\n\n\nfrom sematch.semantic.graph import SimGraph\nfrom sematch.semantic.similarity import WordNetSimilarity\nfrom sematch.nlp import Extraction, word_process\nfrom sematch.semantic.sparql import EntityFeatures\nfrom collections import Counter\ntom = EntityFeatures().features('http://dbpedia.org/resource/Tom_Cruise')\nwords = Extraction().extract_nouns(tom['abstract'])\nwords = word_process(words)\nwns = WordNetSimilarity()\nword_graph = SimGraph(words, wns.word_similarity)\nword_scores = word_graph.page_rank()\nwords, scores =zip(*Counter(word_scores).most_common(10))\nprint words\n(u'picture', u'action', u'number', u'film', u'post', u'sport', \nu'program', u'men', u'performance', u'motion')\n\n\n\n\n\n\nPublications\n\n\n\n\nGanggao Zhu and Carlos A. Iglesias \nComputing Semantic Similarity of Concepts in Knowledge Graphs\n, TKDE, 2016.\n\n\nOscar Araque, Ganggao Zhu, Manuel Garcia-Amado and Carlos A. Iglesias \nMining the Opinionated Web: Classification and Detection of Aspect Contexts for Aspect Based Sentiment Analysis\n,  ICDM sentire, 2016.\n\n\nGanggao Zhu and Carlos A. Iglesias \nSematch: Semantic Entity Search from Knowledge Graph\n\n  ESWC SumPre 2015\n\n\n\n\n\n\nSupport\n\n\nYou can post bug reports and feature requests in \nGithub issues\n. \nThe project is mainly maintained by Ganggao Zhu. You can contact him via gzhu [at] dit.upm.es\n\n\n\n\nWhy this name, Sematch and Logo?\n\n\nThe name of Sematch is composed based on Spanish \"se\" and English \"match\". It is also the abbreviation of semantic matching because semantic similarity metrics help to determine semantic distance of concepts, words, entities, instead of exact matching.\n\n\nThe logo of Sematch is based on Chinese \nYin and Yang\n which is written in \nI Ching\n. Somehow, it correlates to 0 and 1 in computer science.", 
            "title": "Home"
        }, 
        {
            "location": "/#introduction", 
            "text": "Sematch is an integrated framework for the development, evaluation, and application of semantic similarity for Knowledge Graphs (KGs). It is easy to use Sematch to compute semantic similarity scores of concepts, words and entities. Sematch focuses on specific knowledge-based semantic similarity metrics that rely on structural knowledge in taxonomy (e.g. depth, path length, least common subsumer), and statistical information contents (corpus-IC and graph-IC). Knowledge-based approaches are different from their conterpart corpus-based approaches, which are mainly based on word coocurrence (e.g. Pointwise Mutual Information) or distributional semantics (Latent Semantic Analysis, Word2Vec, GLOVE and etc). Knowledge-based approaches are usually used for structual KGs, while corpus-based approaches are normally applied in textual corpora.  In text analysis applications, a common pipeline is adolpted in using semantic similarity from concept level, to word and sentence level. For example, word similarity is first computed based on similarity scores of WordNet concepts, and sentence similarity is computed by composing word similarity scores. Finally, document similarity could be computed by identifying important sentences, e.g. TextRank.     KG based applications also meet similar pipeline in using semantic similarity, from concept similarity (e.g.  http://dbpedia.org/class/yago/Actor109765278 ) to entity similarity (e.g.  http://dbpedia.org/resource/Madrid ). Furthermore, in computing document similarity, entities are extracted and document similarity is computed by composing entity similarity scores.    In KGs, concepts usually denote ontology classes while entities refer to ontology instances. Moreover, those concepts are usually constructed into hierarchical taxonomies, such as DBpedia ontology class, thus quantifying concept similarity in KG relies on similar semantic information (e.g. path length,  depth, least common subsumer, information content) and semantic similarity metrics (e.g. Path, Wu   Palmer,Li,  Resnik, Lin, Jiang   Conrad and WPath). In consequence,  Sematch provides an integrated framework to develop and evaluate semantic similarity metrics for concepts, words, entities and their applications.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#getting-started-20-minutes-to-sematch", 
            "text": "Install Sematch  You need to install scientific computing libraries  numpy  and  scipy  first. An example of installing them with pip is shown below.  pip install numpy scipy  Depending on different OS, you can use different ways to install them. After sucessful installation of  numpy  and  scipy , you can install sematch with following commands.  pip install sematch\npython -m sematch.download  Alternatively, you can use the development version to clone and install Sematch with setuptools. We recommend you to update your pip and setuptools.  git clone https://github.com/gsi-upm/sematch.git\ncd sematch\npython setup.py install  We also provide a  Sematch-Demo Server . You can use it for experimenting with main functionalities or take it as an example for using Sematch to develop applications. Please check our  Documentation  for more details.  Computing Word Similarity .  The core module of Sematch is measuring semantic similarity between concepts that are represented as concept taxonomies. Word similarity is computed based on the maximum semantic similarity of WordNet concepts. You can use Sematch to compute multilingual word similarity based on WordNet with various of semantic similarity metrics.  from sematch.semantic.similarity import WordNetSimilarity\nwns = WordNetSimilarity()\n\n# Computing English word similarity using Li method\nwns.word_similarity('dog', 'cat', 'li') # 0.449327301063\n# Computing Spanish word similarity using Lin method\nwns.monol_word_similarity('perro', 'gato', 'spa', 'lin') #0.876800984373\n# Computing Chinese word similarity using  Wu   Palmer method\nwns.monol_word_similarity('\u72d7', '\u732b', 'cmn', 'wup') # 0.857142857143\n# Computing Spanish and English word similarity using Resnik method\nwns.crossl_word_similarity('perro', 'cat', 'spa', 'eng', 'res') #7.91166650904\n# Computing Spanish and Chinese word similarity using Jiang   Conrad method\nwns.crossl_word_similarity('perro', '\u732b', 'spa', 'cmn', 'jcn') #0.31023804699\n# Computing Chinese and English word similarity using WPath method\nwns.crossl_word_similarity('\u72d7', 'cat', 'cmn', 'eng', 'wpath')#0.593666388463  Computing semantic similarity of YAGO concepts .  from sematch.semantic.similarity import YagoTypeSimilarity\nsim = YagoTypeSimilarity()\n\n#Measuring YAGO concept similarity through WordNet taxonomy and corpus based information content\nsim.yago_similarity('http://dbpedia.org/class/yago/Dancer109989502','http://dbpedia.org/class/yago/Actor109765278', 'wpath') #0.642\nsim.yago_similarity('http://dbpedia.org/class/yago/Dancer109989502','http://dbpedia.org/class/yago/Singer110599806', 'wpath') #0.544\n#Measuring YAGO concept similarity based on graph-based IC\nsim.yago_similarity('http://dbpedia.org/class/yago/Dancer109989502','http://dbpedia.org/class/yago/Actor109765278', 'wpath_graph') #0.423\nsim.yago_similarity('http://dbpedia.org/class/yago/Dancer109989502','http://dbpedia.org/class/yago/Singer110599806', 'wpath_graph') #0.328  Computing semantic similarity of DBpedia concepts .  from sematch.semantic.graph import DBpediaDataTransform, Taxonomy\nfrom sematch.semantic.similarity import ConceptSimilarity\nconcept = ConceptSimilarity(Taxonomy(DBpediaDataTransform()),'models/dbpedia_type_ic.txt')\nconcept.name2concept('actor')\nconcept.similarity('http://dbpedia.org/ontology/Actor','http://dbpedia.org/ontology/Film', 'path')\nconcept.similarity('http://dbpedia.org/ontology/Actor','http://dbpedia.org/ontology/Film', 'wup')\nconcept.similarity('http://dbpedia.org/ontology/Actor','http://dbpedia.org/ontology/Film', 'li')\nconcept.similarity('http://dbpedia.org/ontology/Actor','http://dbpedia.org/ontology/Film', 'res')\nconcept.similarity('http://dbpedia.org/ontology/Actor','http://dbpedia.org/ontology/Film', 'lin')\nconcept.similarity('http://dbpedia.org/ontology/Actor','http://dbpedia.org/ontology/Film', 'jcn')\nconcept.similarity('http://dbpedia.org/ontology/Actor','http://dbpedia.org/ontology/Film', 'wpath')  Computing semantic similarity of DBpedia entities .  from sematch.semantic.similarity import EntitySimilarity\nsim = EntitySimilarity()\nsim.similarity('http://dbpedia.org/resource/Madrid','http://dbpedia.org/resource/Barcelona') #0.409923677282\nsim.similarity('http://dbpedia.org/resource/Apple_Inc.','http://dbpedia.org/resource/Steve_Jobs')#0.0904545454545\nsim.relatedness('http://dbpedia.org/resource/Madrid','http://dbpedia.org/resource/Barcelona')#0.457984139871\nsim.relatedness('http://dbpedia.org/resource/Apple_Inc.','http://dbpedia.org/resource/Steve_Jobs')#0.465991132787  Evaluate semantic similarity metrics with word similarity datasets .  from sematch.evaluation import WordSimEvaluation\nfrom sematch.semantic.similarity import WordNetSimilarity\nevaluation = WordSimEvaluation()\nevaluation.dataset_names()\nwns = WordNetSimilarity()\n# define similarity metrics\nwpath = lambda x, y: wns.word_similarity_wpath(x, y, 0.8)\n# evaluate similarity metrics with SimLex dataset\nevaluation.evaluate_metric('wpath', wpath, 'noun_simlex')\n# performa Steiger's Z significance Test\nevaluation.statistical_test('wpath', 'path', 'noun_simlex')\n# define similarity metrics for Spanish words\nwpath_es = lambda x, y: wns.monol_word_similarity(x, y, 'spa', 'path')\n# define cross-lingual similarity metrics for English-Spanish\nwpath_en_es = lambda x, y: wns.crossl_word_similarity(x, y, 'eng', 'spa', 'wpath')\n# evaluate metrics in multilingual word similarity datasets\nevaluation.evaluate_metric('wpath_es', wpath_es, 'rg65_spanish')\nevaluation.evaluate_metric('wpath_en_es', wpath_en_es, 'rg65_EN-ES')  Evaluate semantic similarity metrics with category classification .  Although the word similarity correlation measure is the standard way to evaluate the semantic similarity metrics, it relies on human judgements over word pairs which may not have same performance in real applications. Therefore, apart from word similarity evaluation, the Sematch evaluation framework also includes a simple aspect category classification. The task classifies noun concepts such as pasta, noodle, steak, tea into their ontological parent concept FOOD, DRINKS.  from sematch.evaluation import AspectEvaluation\nfrom sematch.application import SimClassifier, SimSVMClassifier\nfrom sematch.semantic.similarity import WordNetSimilarity\n\n# create aspect classification evaluation\nevaluation = AspectEvaluation()\n# load the dataset\nX, y = evaluation.load_dataset()\n# define word similarity function\nwns = WordNetSimilarity()\nword_sim = lambda x, y: wns.word_similarity(x, y)\n# Train and evaluate metrics with unsupervised classification model\nsimclassifier = SimClassifier.train(zip(X,y), word_sim)\nevaluation.evaluate(X,y, simclassifier)\n\nmacro averge:  (0.65319812882333839, 0.7101245049198579, 0.66317566364913016, None)\nmicro average:  (0.79210167952791644, 0.79210167952791644, 0.79210167952791644, None)\nweighted average:  (0.80842645056024054, 0.79210167952791644, 0.79639496616636352, None)\naccuracy:  0.792101679528\n             precision    recall  f1-score   support\n\n    SERVICE       0.50      0.43      0.46       519\n RESTAURANT       0.81      0.66      0.73       228\n       FOOD       0.95      0.87      0.91      2256\n   LOCATION       0.26      0.67      0.37        54\n   AMBIENCE       0.60      0.70      0.65       597\n     DRINKS       0.81      0.93      0.87       752\n\navg / total       0.81      0.79      0.80      4406  You can use Sematch to download a list of entities having a specific type using different languages. Sematch will generate SPARQL queries and execute them in  DBpedia Sparql Endpoint .  from sematch.application import Matcher\nmatcher = Matcher()\n# matching scientist entities from DBpedia\nmatcher.match_type('scientist')\nmatcher.match_type('cient\u00edfico', 'spa')\nmatcher.match_type('\u79d1\u5b66\u5bb6', 'cmn')\n\n\n----SPARQL queries----\nSELECT DISTINCT ?s, ?label, ?abstract WHERE {\n    {  \n    ?s  http://www.w3.org/1999/02/22-rdf-syntax-ns#type   http://dbpedia.org/class/yago/NuclearPhysicist110364643  . }\n UNION {  \n    ?s  http://www.w3.org/1999/02/22-rdf-syntax-ns#type   http://dbpedia.org/class/yago/Econometrician110043491  . }\n UNION {  \n    ?s  http://www.w3.org/1999/02/22-rdf-syntax-ns#type   http://dbpedia.org/class/yago/Sociologist110620758  . }\n UNION {  \n    ?s  http://www.w3.org/1999/02/22-rdf-syntax-ns#type   http://dbpedia.org/class/yago/Archeologist109804806  . }\n UNION {  \n    ?s  http://www.w3.org/1999/02/22-rdf-syntax-ns#type   http://dbpedia.org/class/yago/Neurolinguist110354053  . } \n    ?s  http://www.w3.org/1999/02/22-rdf-syntax-ns#type   http://www.w3.org/2002/07/owl#Thing  . \n    ?s  http://www.w3.org/2000/01/rdf-schema#label  ?label . \n    FILTER( lang(?label) =  en ) . \n    ?s  http://dbpedia.org/ontology/abstract  ?abstract . \n    FILTER( lang(?abstract) =  en ) .\n} LIMIT 5000  Matching Entities with type using SPARQL queries .  You can use Sematch to download a list of entities having a specific type using different languages. Sematch will generate SPARQL queries and execute them in  DBpedia Sparql Endpoint .  from sematch.application import Matcher\nmatcher = Matcher()\n# matching scientist entities from DBpedia\nmatcher.match_type('scientist')\nmatcher.match_type('cient\u00edfico', 'spa')\nmatcher.match_type('\u79d1\u5b66\u5bb6', 'cmn')\nmatcher.match_entity_type('movies with Tom Cruise')  Example of automatically generated SPARQL query.  SELECT DISTINCT ?s, ?label, ?abstract WHERE {\n    {  \n    ?s  http://www.w3.org/1999/02/22-rdf-syntax-ns#type   http://dbpedia.org/class/yago/NuclearPhysicist110364643  . }\n UNION {  \n    ?s  http://www.w3.org/1999/02/22-rdf-syntax-ns#type   http://dbpedia.org/class/yago/Econometrician110043491  . }\n UNION {  \n    ?s  http://www.w3.org/1999/02/22-rdf-syntax-ns#type   http://dbpedia.org/class/yago/Sociologist110620758  . }\n UNION {  \n    ?s  http://www.w3.org/1999/02/22-rdf-syntax-ns#type   http://dbpedia.org/class/yago/Archeologist109804806  . }\n UNION {  \n    ?s  http://www.w3.org/1999/02/22-rdf-syntax-ns#type   http://dbpedia.org/class/yago/Neurolinguist110354053  . } \n    ?s  http://www.w3.org/1999/02/22-rdf-syntax-ns#type   http://www.w3.org/2002/07/owl#Thing  . \n    ?s  http://www.w3.org/2000/01/rdf-schema#label  ?label . \n    FILTER( lang(?label) =  en ) . \n    ?s  http://dbpedia.org/ontology/abstract  ?abstract . \n    FILTER( lang(?abstract) =  en ) .\n} LIMIT 5000  Entity feature extraction with Similarity Graph  Apart from semantic matching of entities from DBpedia, you can also use Sematch to extract features of entities and apply semantic similarity analysis using graph-based ranking algorithms. Given a list of objects (concepts, words, entities), Sematch compute their pairwise semantic similarity and generate similarity graph where nodes denote objects and edges denote similarity scores. An example of using similarity graph for extracting important words from an entity description.  from sematch.semantic.graph import SimGraph\nfrom sematch.semantic.similarity import WordNetSimilarity\nfrom sematch.nlp import Extraction, word_process\nfrom sematch.semantic.sparql import EntityFeatures\nfrom collections import Counter\ntom = EntityFeatures().features('http://dbpedia.org/resource/Tom_Cruise')\nwords = Extraction().extract_nouns(tom['abstract'])\nwords = word_process(words)\nwns = WordNetSimilarity()\nword_graph = SimGraph(words, wns.word_similarity)\nword_scores = word_graph.page_rank()\nwords, scores =zip(*Counter(word_scores).most_common(10))\nprint words\n(u'picture', u'action', u'number', u'film', u'post', u'sport', \nu'program', u'men', u'performance', u'motion')", 
            "title": "Getting started: 20 minutes to Sematch"
        }, 
        {
            "location": "/#publications", 
            "text": "Ganggao Zhu and Carlos A. Iglesias  Computing Semantic Similarity of Concepts in Knowledge Graphs , TKDE, 2016.  Oscar Araque, Ganggao Zhu, Manuel Garcia-Amado and Carlos A. Iglesias  Mining the Opinionated Web: Classification and Detection of Aspect Contexts for Aspect Based Sentiment Analysis ,  ICDM sentire, 2016.  Ganggao Zhu and Carlos A. Iglesias  Sematch: Semantic Entity Search from Knowledge Graph \n  ESWC SumPre 2015", 
            "title": "Publications"
        }, 
        {
            "location": "/#support", 
            "text": "You can post bug reports and feature requests in  Github issues . \nThe project is mainly maintained by Ganggao Zhu. You can contact him via gzhu [at] dit.upm.es", 
            "title": "Support"
        }, 
        {
            "location": "/#why-this-name-sematch-and-logo", 
            "text": "The name of Sematch is composed based on Spanish \"se\" and English \"match\". It is also the abbreviation of semantic matching because semantic similarity metrics help to determine semantic distance of concepts, words, entities, instead of exact matching.  The logo of Sematch is based on Chinese  Yin and Yang  which is written in  I Ching . Somehow, it correlates to 0 and 1 in computer science.", 
            "title": "Why this name, Sematch and Logo?"
        }, 
        {
            "location": "/similarity/", 
            "text": "Word Similarity\n\n\nThe key module of Sematch is measuring semantic similarity based on taxonomies. The word similarity is computed based on WordNet taxonomy with various semantic similarity metrics. Sematch extends the NLTK version of WordNet and similarities with lemmatization, multilingual support \nOpen Multilingual WordNet\n and more semantic similarity metrics into a single class \nWordNetSimilarity\n. NLTK provides path, lch, wup, res, lin, and jcn, while jcn metric has divide zero problem. In Sematch, we provide uniformed metric interfaces and include two more metrics li and wpath. Those metric names and their corresponding publications are listed as below.\n\n\n\n\npath\n\n  Rada, Roy, et al. \"Development and application of a metric on semantic nets.\" IEEE transactions on systems, man, and cybernetics 19.1 (1989): 17-30.\n\n\nlch\n\n  Leacock, Claudia, and Martin Chodorow. \"Combining local context and WordNet similarity for word sense identification.\" WordNet: An electronic lexical database 49.2 (1998): 265-283.\n\n\nwup\n\n  Wu, Zhibiao, and Martha Palmer. \"Verbs semantics and lexical selection.\" Proceedings of the 32nd annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, 1994.\n\n\nli\n\n  Li, Yuhua, Zuhair A. Bandar, and David McLean. \"An approach for measuring semantic similarity between words using multiple information sources.\" IEEE Transactions on knowledge and data engineering 15.4 (2003): 871-882.\n\n\nres\n\n  Resnik, Philip. \"Using information content to evaluate semantic similarity in a taxonomy.\" arXiv preprint cmp-lg/9511007 (1995).\n\n\nlin\n\n  Lin, Dekang. \"An information-theoretic definition of similarity.\" ICML. Vol. 98. No. 1998. 1998.\n\n\njcn\n\n  Jiang, Jay J., and David W. Conrath. \"Semantic similarity based on corpus statistics and lexical taxonomy.\" arXiv preprint cmp-lg/9709008 (1997).\n\n\nwpath\n\n  Ganggao Zhu, and Carlos A. Iglesias. \"Computing Semantic Similarity of Concepts in Knowledge Graphs.\" IEEE Transactions on Knowledge and Data Engineering 29.1 (2017): 72-85.\n\n\n\n\nYou can compute multilingual word similarity with various semantic similarity metrics using WordNet. Some examples are shown as below.\n\n\nfrom sematch.semantic.similarity import WordNetSimilarity\nwns = WordNetSimilarity()\n# Computing English word similarity using Li method\nprint wns.word_similarity('dog', 'cat', 'li') # 0.449327301063\n\n# Computing Spanish word similarity using Lin method\nprint wns.monol_word_similarity('perro', 'gato', 'spa', 'lin') #0.876800984373\n\n# Computing Chinese word similarity using  Wu \n Palmer method\nprint wns.monol_word_similarity('\u72d7', '\u732b', 'cmn', 'wup') # 0.857142857143\n\n# Computing Spanish and English word similarity using Resnik method\nprint wns.crossl_word_similarity('perro', 'cat', 'spa', 'eng', 'res') #7.91166650904\n\n# Computing Spanish and Chinese word similarity using Jiang \n Conrad method\nprint wns.crossl_word_similarity('perro', '\u732b', 'spa', 'cmn', 'jcn') #0.31023804699\n\n# Computing Chinese and English word similarity using WPath method\nprint wns.crossl_word_similarity('\u72d7', 'cat', 'cmn', 'eng', 'wpath')#0.593666388463\n\n\n\n\n\nThe multilingual word similarity is supported by Open Multilingual WordNet and the details are described in the \nproject documentation\n . We have provided a function to view a list of supported languages and find the corresponding language code. The function supports both captial and lower cased string.\n\n\n#check the supported languages\nprint wns.languages()\n['Galicain', 'Portuguese', 'Spanish', 'Chinese_traditional', 'Japanese', 'Persian', 'Slovak', 'Hebrew', 'Polish', 'Arabic', 'Swedish', 'Icelandic', 'Romanian', 'Dutch', 'Danish', 'Bulgarian', 'Lithuanian', 'Malay', 'French', 'Catalan', 'Thai', 'Chinese_simplified', 'Basque', 'Slovene', 'Finnish', 'Albanian', 'Greek', 'Indonesian', 'English', 'Croatian', 'Italian']\n#find the language code\nprint wns.languages('English')\nprint wns.languages('chinese_simplified')\nprint wns.languages('spanish')\neng\ncmn\nspa\n\n\n\n\nYou can compute pairwised semantic similarity between a list of words. We show an example for illustration.\n\n\nwns = WordNetSimilarity()\nwords = ['artist', 'musician', 'scientist', 'physicist', 'actor', 'movie']\nsim_matrix = [[wns.word_similarity(w1, w2, 'wpath') for w1 in words] for w2 in words]\n\n\n\n\n\n\n\n\n\n\n\n\nartist\n\n\nmusician\n\n\nscientist\n\n\nphysicist\n\n\nactor\n\n\nmovie\n\n\n\n\n\n\n\n\n\n\nartist\n\n\n1.000000\n\n\n0.809924\n\n\n0.359417\n\n\n0.296175\n\n\n0.359417\n\n\n0.135239\n\n\n\n\n\n\nmusician\n\n\n0.809924\n\n\n1.000000\n\n\n0.296175\n\n\n0.251859\n\n\n0.641697\n\n\n0.123384\n\n\n\n\n\n\nscientist\n\n\n0.359417\n\n\n0.296175\n\n\n1.000000\n\n\n0.790743\n\n\n0.456999\n\n\n0.149615\n\n\n\n\n\n\nphysicist\n\n\n0.296175\n\n\n0.251859\n\n\n0.790743\n\n\n1.000000\n\n\n0.359417\n\n\n0.135239\n\n\n\n\n\n\nactor\n\n\n0.359417\n\n\n0.641697\n\n\n0.456999\n\n\n0.359417\n\n\n1.000000\n\n\n0.149615\n\n\n\n\n\n\nmovie\n\n\n0.135239\n\n\n0.123384\n\n\n0.149615\n\n\n0.135239\n\n\n0.149615\n\n\n1.000000\n\n\n\n\n\n\n\n\nYAGO Concept Similarity\n\n\nApart from word similarity, \nYagoTypeSimilarity\n extends the \nWordNetSimilarity\n to compute semantic similarity between YAGO concepts (e.g. \nhttp://dbpedia.org/class/yago/Actor109765278\n). In order to achieve this, Sematch created WordNet synset and YAGO concept mappings based on the \nYAGO dataset\n. Using the synset version in WordNet for YAGO concepts has three main advantages. First, it is convenient and fast to explore taxonomy structure and derive structural information (depth, path length, least common subsumer) using existing WordNet taxonomy in NLTK. Note that YAGO concepts are originally created based on WordNet synsets. Second, existing information contents computed from SemCor or Brown Corpus can be directly used to compute YAGO concept similarity with metrics such as res, lin, jcn and wpath. Finally, words can be mapped to YAGO concepts easily which is useful for semantic matching applications. \n\n\nApart from the existing metrics, in \nYagoTypeSimilarity\n, we also implemented those metrics res, lin, jcn and wpath with graph-based information content that is computed from YAGO concept's frequency in DBpedia by executing SPARQL queries to DBpedia endpoint. In summary, \nYagoTypeSimilarity\n maps YAGO concepts to WordNet synsets and computes their semantic similarity using WordNet taxonomy and either corpus-based or graph-based information content. We give some examples for illustration.\n\n\nfrom sematch.semantic.similarity import YagoTypeSimilarity\nyago_sim = YagoTypeSimilarity()\n\n#Mapping a word to yago links in DBpedia\ndancer = yago_sim.word2yago('dancer')# e.g. 'http://dbpedia.org/class/yago/Dancer109989502'\nactor = yago_sim.word2yago('actor') # e.g. 'http://dbpedia.org/class/yago/Actor109765278'\nsinger = yago_sim.word2yago('singer')# e.g. 'http://dbpedia.org/class/yago/Singer110599806'\n\n#Mapping a yago link to WordNet synset\nprint yago_sim.yago2synset('http://dbpedia.org/class/yago/Actor109765278') \nSynset('actor.n.01')\n\n#Measuring YAGO concept similarity through WordNet taxonomy and corpus based information content\nyago_sim.yago_similarity('http://dbpedia.org/class/yago/Dancer109989502', \n                        'http://dbpedia.org/class/yago/Actor109765278', 'wpath')\n0.642\n\nyago_sim.yago_similarity('http://dbpedia.org/class/yago/Dancer109989502',\n                        'http://dbpedia.org/class/yago/Singer110599806', 'wpath')\n0.544\n\n#Measuring YAGO concept similarity based on graph-based IC \nyago_sim.yago_similarity('http://dbpedia.org/class/yago/Dancer109989502', \n                        'http://dbpedia.org/class/yago/Actor109765278', 'wpath_graph')\n0.423\n\nyago_sim.yago_similarity('http://dbpedia.org/class/yago/Dancer109989502',\n                        'http://dbpedia.org/class/yago/Singer110599806', 'wpath_graph')\n0.328\n\n\n\n\nDBpedia Concept Similarity\n\n\nSince YAGO concepts have corresponding mappings to WordNet synsets, we can reuse exsiting codes for computing semantic similarity of YAGO concepts. However, DBpedia concepts (ontology classes) in DBpedia ontology does not have such mappings, so we created \nTaxonomy\n class to parse such kind of concept taxonomy and implemented \nConceptSimilarity\n class with all the semantic similarity metrics used in WordNet taxonomy. Since concept taxonomies such as DBpedia concept taxonomy does not have concept-annotated corpus like SemCor for WordNet, those semantic similarity metrics based on information contents can not be used. In order to solve this problem, we implemented graph-based information content (\nGraphIC\n class) relies on concept distribution in Knowledge Graph like DBpedia. For more technical details, user should refer the following article.\n\n\n\n\nGanggao Zhu, and Carlos A. Iglesias. \"Computing Semantic Similarity of Concepts in Knowledge Graphs.\" IEEE Transactions on Knowledge and Data Engineering 29.1 (2017): 72-85.\n\n\n\n\nThen you can parse DBpedia concept data to \nTaxonomy\n and \nConceptSimilarity\n for computing semantic similarity between DBpedia concepts.\n\n\nfrom sematch.semantic.graph import DBpediaDataTransform, Taxonomy\nfrom sematch.semantic.similarity import ConceptSimilarity\nconcept = ConceptSimilarity(Taxonomy(DBpediaDataTransform()), 'models/dbpedia_type_ic.txt')\nprint concept.name2concept('actor')\nprint concept.similarity('http://dbpedia.org/ontology/Actor', 'http://dbpedia.org/ontology/Film', 'path')\nprint concept.similarity('http://dbpedia.org/ontology/Actor', 'http://dbpedia.org/ontology/Film', 'wup')\nprint concept.similarity('http://dbpedia.org/ontology/Actor', 'http://dbpedia.org/ontology/Film', 'li')\nprint concept.similarity('http://dbpedia.org/ontology/Actor', 'http://dbpedia.org/ontology/Film', 'res')\nprint concept.similarity('http://dbpedia.org/ontology/Actor', 'http://dbpedia.org/ontology/Film', 'lin')\nprint concept.similarity('http://dbpedia.org/ontology/Actor', 'http://dbpedia.org/ontology/Film', 'jcn')\nprint concept.similarity('http://dbpedia.org/ontology/Actor', 'http://dbpedia.org/ontology/Film', 'wpath')\n\n\n\n\nDBpedia Entity Similarity\n\n\nWe provide two methods to measure entity similarity. One is \nentity relatedness\n which is based on DBpedia link association which mainly measures entity link overlap between entities. You can check the detail of the method in the following article\n\n\n\n\n\n\nentity relatedness\n Milne, David, and Ian H. Witten. \"Learning to link with wikipedia.\" Proceedings of the 17th ACM conference on Information and knowledge management. ACM, 2008.\n  \u200b                                                \n\nAnother one is \nentity similarity\n measure which is based on YAGO concept similarity. First YAGO concepts of entity is extracted from DBpedia using \nEntityFeatures\n class. Then, top 5 concets with highest graph-based information contents are selected and composed as concept list. Finally, we compute semantic similarity of two entities by calculating semantic similarity of two concept lists, which is similar to compute text similarity based two word lists. You can check the function details in the following publication.\n\n\n\n\n\n\nentity similarity\n Mihalcea, Rada, Courtney Corley, and Carlo Strapparava. \"Corpus-based and knowledge-based measures of text semantic similarity.\" AAAI. Vol. 6. 2006.\n\n\n\n\n\n\nfrom sematch.semantic.similarity import EntitySimilarity\nentity_sim = EntitySimilarity()\nprint entity_sim.similarity('http://dbpedia.org/resource/Madrid',\n                            'http://dbpedia.org/resource/Barcelona')\n0.409923677282\n\nprint entity_sim.similarity('http://dbpedia.org/resource/Apple_Inc.',\n                            'http://dbpedia.org/resource/Steve_Jobs')\n0.0904545454545\n\nprint entity_sim.relatedness('http://dbpedia.org/resource/Madrid', \n                             'http://dbpedia.org/resource/Barcelona')\n0.457984139871\n\nprint entity_sim.relatedness('http://dbpedia.org/resource/Apple_Inc.',\n                             'http://dbpedia.org/resource/Steve_Jobs')\n0.465991132787\n\n\n\n\nYou can see from the example that the \nentity similarity\n gives very low similarity score to entity \nhttp://dbpedia.org/resource/Apple_Inc.\n and entity \nhttp://dbpedia.org/resource/Steve_Jobs\n because semantic similarity measures entity's taxonomical similarity. Apple_Inc is a company and Steve_Jobs is a person. Although they are clearly related, they are not similar type.\n\n\nThe similarity method is our focus which is based on the semantic similarity of concepts, while the relatedness method is based on degree analysis (incoming and outgoing links). The similarity method computes faster because it only needs to run two SPARQL queries to obtain the required features which are lists of entity concepts (e.g. movie, actor). The relatedness method need more time since it needs at least 6 SPARQL queries to count the incoming and outcoming links of two entities. The main bottleneck lies in feature extraction through SPARQL. We think Sematch offers a convenient way for research purpose in small dataset to compute entity similarity and relatedness, where users can store the computed results and Sematch offers cache using memoized function so Sematch will return the computed results directly. Thus, it is only slow the first time is computed, that can be acceptable in a number of scenarios. However, user should consider other ways to extract entity features for efficiency consideration.", 
            "title": "Similarity"
        }, 
        {
            "location": "/similarity/#word-similarity", 
            "text": "The key module of Sematch is measuring semantic similarity based on taxonomies. The word similarity is computed based on WordNet taxonomy with various semantic similarity metrics. Sematch extends the NLTK version of WordNet and similarities with lemmatization, multilingual support  Open Multilingual WordNet  and more semantic similarity metrics into a single class  WordNetSimilarity . NLTK provides path, lch, wup, res, lin, and jcn, while jcn metric has divide zero problem. In Sematch, we provide uniformed metric interfaces and include two more metrics li and wpath. Those metric names and their corresponding publications are listed as below.   path \n  Rada, Roy, et al. \"Development and application of a metric on semantic nets.\" IEEE transactions on systems, man, and cybernetics 19.1 (1989): 17-30.  lch \n  Leacock, Claudia, and Martin Chodorow. \"Combining local context and WordNet similarity for word sense identification.\" WordNet: An electronic lexical database 49.2 (1998): 265-283.  wup \n  Wu, Zhibiao, and Martha Palmer. \"Verbs semantics and lexical selection.\" Proceedings of the 32nd annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, 1994.  li \n  Li, Yuhua, Zuhair A. Bandar, and David McLean. \"An approach for measuring semantic similarity between words using multiple information sources.\" IEEE Transactions on knowledge and data engineering 15.4 (2003): 871-882.  res \n  Resnik, Philip. \"Using information content to evaluate semantic similarity in a taxonomy.\" arXiv preprint cmp-lg/9511007 (1995).  lin \n  Lin, Dekang. \"An information-theoretic definition of similarity.\" ICML. Vol. 98. No. 1998. 1998.  jcn \n  Jiang, Jay J., and David W. Conrath. \"Semantic similarity based on corpus statistics and lexical taxonomy.\" arXiv preprint cmp-lg/9709008 (1997).  wpath \n  Ganggao Zhu, and Carlos A. Iglesias. \"Computing Semantic Similarity of Concepts in Knowledge Graphs.\" IEEE Transactions on Knowledge and Data Engineering 29.1 (2017): 72-85.   You can compute multilingual word similarity with various semantic similarity metrics using WordNet. Some examples are shown as below.  from sematch.semantic.similarity import WordNetSimilarity\nwns = WordNetSimilarity()\n# Computing English word similarity using Li method\nprint wns.word_similarity('dog', 'cat', 'li') # 0.449327301063\n\n# Computing Spanish word similarity using Lin method\nprint wns.monol_word_similarity('perro', 'gato', 'spa', 'lin') #0.876800984373\n\n# Computing Chinese word similarity using  Wu   Palmer method\nprint wns.monol_word_similarity('\u72d7', '\u732b', 'cmn', 'wup') # 0.857142857143\n\n# Computing Spanish and English word similarity using Resnik method\nprint wns.crossl_word_similarity('perro', 'cat', 'spa', 'eng', 'res') #7.91166650904\n\n# Computing Spanish and Chinese word similarity using Jiang   Conrad method\nprint wns.crossl_word_similarity('perro', '\u732b', 'spa', 'cmn', 'jcn') #0.31023804699\n\n# Computing Chinese and English word similarity using WPath method\nprint wns.crossl_word_similarity('\u72d7', 'cat', 'cmn', 'eng', 'wpath')#0.593666388463  The multilingual word similarity is supported by Open Multilingual WordNet and the details are described in the  project documentation  . We have provided a function to view a list of supported languages and find the corresponding language code. The function supports both captial and lower cased string.  #check the supported languages\nprint wns.languages()\n['Galicain', 'Portuguese', 'Spanish', 'Chinese_traditional', 'Japanese', 'Persian', 'Slovak', 'Hebrew', 'Polish', 'Arabic', 'Swedish', 'Icelandic', 'Romanian', 'Dutch', 'Danish', 'Bulgarian', 'Lithuanian', 'Malay', 'French', 'Catalan', 'Thai', 'Chinese_simplified', 'Basque', 'Slovene', 'Finnish', 'Albanian', 'Greek', 'Indonesian', 'English', 'Croatian', 'Italian']\n#find the language code\nprint wns.languages('English')\nprint wns.languages('chinese_simplified')\nprint wns.languages('spanish')\neng\ncmn\nspa  You can compute pairwised semantic similarity between a list of words. We show an example for illustration.  wns = WordNetSimilarity()\nwords = ['artist', 'musician', 'scientist', 'physicist', 'actor', 'movie']\nsim_matrix = [[wns.word_similarity(w1, w2, 'wpath') for w1 in words] for w2 in words]      artist  musician  scientist  physicist  actor  movie      artist  1.000000  0.809924  0.359417  0.296175  0.359417  0.135239    musician  0.809924  1.000000  0.296175  0.251859  0.641697  0.123384    scientist  0.359417  0.296175  1.000000  0.790743  0.456999  0.149615    physicist  0.296175  0.251859  0.790743  1.000000  0.359417  0.135239    actor  0.359417  0.641697  0.456999  0.359417  1.000000  0.149615    movie  0.135239  0.123384  0.149615  0.135239  0.149615  1.000000", 
            "title": "Word Similarity"
        }, 
        {
            "location": "/similarity/#yago-concept-similarity", 
            "text": "Apart from word similarity,  YagoTypeSimilarity  extends the  WordNetSimilarity  to compute semantic similarity between YAGO concepts (e.g.  http://dbpedia.org/class/yago/Actor109765278 ). In order to achieve this, Sematch created WordNet synset and YAGO concept mappings based on the  YAGO dataset . Using the synset version in WordNet for YAGO concepts has three main advantages. First, it is convenient and fast to explore taxonomy structure and derive structural information (depth, path length, least common subsumer) using existing WordNet taxonomy in NLTK. Note that YAGO concepts are originally created based on WordNet synsets. Second, existing information contents computed from SemCor or Brown Corpus can be directly used to compute YAGO concept similarity with metrics such as res, lin, jcn and wpath. Finally, words can be mapped to YAGO concepts easily which is useful for semantic matching applications.   Apart from the existing metrics, in  YagoTypeSimilarity , we also implemented those metrics res, lin, jcn and wpath with graph-based information content that is computed from YAGO concept's frequency in DBpedia by executing SPARQL queries to DBpedia endpoint. In summary,  YagoTypeSimilarity  maps YAGO concepts to WordNet synsets and computes their semantic similarity using WordNet taxonomy and either corpus-based or graph-based information content. We give some examples for illustration.  from sematch.semantic.similarity import YagoTypeSimilarity\nyago_sim = YagoTypeSimilarity()\n\n#Mapping a word to yago links in DBpedia\ndancer = yago_sim.word2yago('dancer')# e.g. 'http://dbpedia.org/class/yago/Dancer109989502'\nactor = yago_sim.word2yago('actor') # e.g. 'http://dbpedia.org/class/yago/Actor109765278'\nsinger = yago_sim.word2yago('singer')# e.g. 'http://dbpedia.org/class/yago/Singer110599806'\n\n#Mapping a yago link to WordNet synset\nprint yago_sim.yago2synset('http://dbpedia.org/class/yago/Actor109765278') \nSynset('actor.n.01')\n\n#Measuring YAGO concept similarity through WordNet taxonomy and corpus based information content\nyago_sim.yago_similarity('http://dbpedia.org/class/yago/Dancer109989502', \n                        'http://dbpedia.org/class/yago/Actor109765278', 'wpath')\n0.642\n\nyago_sim.yago_similarity('http://dbpedia.org/class/yago/Dancer109989502',\n                        'http://dbpedia.org/class/yago/Singer110599806', 'wpath')\n0.544\n\n#Measuring YAGO concept similarity based on graph-based IC \nyago_sim.yago_similarity('http://dbpedia.org/class/yago/Dancer109989502', \n                        'http://dbpedia.org/class/yago/Actor109765278', 'wpath_graph')\n0.423\n\nyago_sim.yago_similarity('http://dbpedia.org/class/yago/Dancer109989502',\n                        'http://dbpedia.org/class/yago/Singer110599806', 'wpath_graph')\n0.328", 
            "title": "YAGO Concept Similarity"
        }, 
        {
            "location": "/similarity/#dbpedia-concept-similarity", 
            "text": "Since YAGO concepts have corresponding mappings to WordNet synsets, we can reuse exsiting codes for computing semantic similarity of YAGO concepts. However, DBpedia concepts (ontology classes) in DBpedia ontology does not have such mappings, so we created  Taxonomy  class to parse such kind of concept taxonomy and implemented  ConceptSimilarity  class with all the semantic similarity metrics used in WordNet taxonomy. Since concept taxonomies such as DBpedia concept taxonomy does not have concept-annotated corpus like SemCor for WordNet, those semantic similarity metrics based on information contents can not be used. In order to solve this problem, we implemented graph-based information content ( GraphIC  class) relies on concept distribution in Knowledge Graph like DBpedia. For more technical details, user should refer the following article.   Ganggao Zhu, and Carlos A. Iglesias. \"Computing Semantic Similarity of Concepts in Knowledge Graphs.\" IEEE Transactions on Knowledge and Data Engineering 29.1 (2017): 72-85.   Then you can parse DBpedia concept data to  Taxonomy  and  ConceptSimilarity  for computing semantic similarity between DBpedia concepts.  from sematch.semantic.graph import DBpediaDataTransform, Taxonomy\nfrom sematch.semantic.similarity import ConceptSimilarity\nconcept = ConceptSimilarity(Taxonomy(DBpediaDataTransform()), 'models/dbpedia_type_ic.txt')\nprint concept.name2concept('actor')\nprint concept.similarity('http://dbpedia.org/ontology/Actor', 'http://dbpedia.org/ontology/Film', 'path')\nprint concept.similarity('http://dbpedia.org/ontology/Actor', 'http://dbpedia.org/ontology/Film', 'wup')\nprint concept.similarity('http://dbpedia.org/ontology/Actor', 'http://dbpedia.org/ontology/Film', 'li')\nprint concept.similarity('http://dbpedia.org/ontology/Actor', 'http://dbpedia.org/ontology/Film', 'res')\nprint concept.similarity('http://dbpedia.org/ontology/Actor', 'http://dbpedia.org/ontology/Film', 'lin')\nprint concept.similarity('http://dbpedia.org/ontology/Actor', 'http://dbpedia.org/ontology/Film', 'jcn')\nprint concept.similarity('http://dbpedia.org/ontology/Actor', 'http://dbpedia.org/ontology/Film', 'wpath')", 
            "title": "DBpedia Concept Similarity"
        }, 
        {
            "location": "/similarity/#dbpedia-entity-similarity", 
            "text": "We provide two methods to measure entity similarity. One is  entity relatedness  which is based on DBpedia link association which mainly measures entity link overlap between entities. You can check the detail of the method in the following article    entity relatedness  Milne, David, and Ian H. Witten. \"Learning to link with wikipedia.\" Proceedings of the 17th ACM conference on Information and knowledge management. ACM, 2008.\n  \u200b                                                 \nAnother one is  entity similarity  measure which is based on YAGO concept similarity. First YAGO concepts of entity is extracted from DBpedia using  EntityFeatures  class. Then, top 5 concets with highest graph-based information contents are selected and composed as concept list. Finally, we compute semantic similarity of two entities by calculating semantic similarity of two concept lists, which is similar to compute text similarity based two word lists. You can check the function details in the following publication.    entity similarity  Mihalcea, Rada, Courtney Corley, and Carlo Strapparava. \"Corpus-based and knowledge-based measures of text semantic similarity.\" AAAI. Vol. 6. 2006.    from sematch.semantic.similarity import EntitySimilarity\nentity_sim = EntitySimilarity()\nprint entity_sim.similarity('http://dbpedia.org/resource/Madrid',\n                            'http://dbpedia.org/resource/Barcelona')\n0.409923677282\n\nprint entity_sim.similarity('http://dbpedia.org/resource/Apple_Inc.',\n                            'http://dbpedia.org/resource/Steve_Jobs')\n0.0904545454545\n\nprint entity_sim.relatedness('http://dbpedia.org/resource/Madrid', \n                             'http://dbpedia.org/resource/Barcelona')\n0.457984139871\n\nprint entity_sim.relatedness('http://dbpedia.org/resource/Apple_Inc.',\n                             'http://dbpedia.org/resource/Steve_Jobs')\n0.465991132787  You can see from the example that the  entity similarity  gives very low similarity score to entity  http://dbpedia.org/resource/Apple_Inc.  and entity  http://dbpedia.org/resource/Steve_Jobs  because semantic similarity measures entity's taxonomical similarity. Apple_Inc is a company and Steve_Jobs is a person. Although they are clearly related, they are not similar type.  The similarity method is our focus which is based on the semantic similarity of concepts, while the relatedness method is based on degree analysis (incoming and outgoing links). The similarity method computes faster because it only needs to run two SPARQL queries to obtain the required features which are lists of entity concepts (e.g. movie, actor). The relatedness method need more time since it needs at least 6 SPARQL queries to count the incoming and outcoming links of two entities. The main bottleneck lies in feature extraction through SPARQL. We think Sematch offers a convenient way for research purpose in small dataset to compute entity similarity and relatedness, where users can store the computed results and Sematch offers cache using memoized function so Sematch will return the computed results directly. Thus, it is only slow the first time is computed, that can be acceptable in a number of scenarios. However, user should consider other ways to extract entity features for efficiency consideration.", 
            "title": "DBpedia Entity Similarity"
        }, 
        {
            "location": "/evaluation/", 
            "text": "Word Similarity Evaluation\n\n\nWe have collected some well known word similarity datasets for evaluating semantic similarity metrics. Several python classes can be used to prepare the dataset and evaluate the metric automatically. The word similarity datasets included in Sematch are listed as below:\n\n\n\n\nRubenstein and Goodenough (RG)\n \n\n\n\n\nHerbert Rubenstein and John B. Goodenough. 1965. Contextual correlates of synonymy. Commun. ACM 8, 10 (October 1965), 627-633. DOI=10.1145/365628.365657 \n\n\n\n\nMiller and Charles (MC)\n \n\n\n\n\nMiller, George A., and Walter G. Charles. \"Contextual correlates of semantic similarity.\" Language and cognitive processes 6.1 (1991): 1-28.\n\n\n\n\nWordsim353 (WS353)\n \n\n\n\n\nLev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin, \"Placing Search in Context: The Concept Revisited\", ACM Transactions on Information Systems, 20(1):116-131, January 2002 \n\n\n\n\nwordsim353 similarity and relatedness (WS353Sim)\n \n\n\n\n\nEneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pasca, Aitor Soroa, A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches, In Proceedings of NAACL-HLT 2009.\n\n\n\n\nSimLex-999 (SIMLEX)\n \n\n\n\n\nSimLex-999: Evaluating Semantic Models with (Genuine) Similarity Estimation. 2014. Felix Hill, Roi Reichart and Anna Korhonen. Preprint pubslished on arXiv. arXiv:1408.3456\n\n\n\n\nMultilingual Word Similarity\n\n\n\n\nCamacho-Collados, Jos\u00e9, Mohammad Taher Pilehvar, and Roberto Navigli. \"A Framework for the Construction of Monolingual and Cross-lingual Word Similarity Datasets.\" ACL (2). 2015.\n\n\nWhen developing new similarity metrics, proper evaluation is important, whereas sometimes it is tedious. Sematch helps to save such efforts by providing a evaluation framework, where similarity methods are evaluated with common word similarity datasets and can be compared with each other. \n\n\nThe most established methodology for evaluating performance of semantic similarity methods in word similarity dataset, is measuring the Spearman correlation between similarity scores generated by the similarity methods and scores assessed by human. Note that both Spearman's and Pearson's correlations coefficients have been commonly used in the literatures. They are equivalent if rating scores are ordered and we use Spearman correlation coefficients as default. A similarity method is acknolwedged to have better performance if it has higher correlation score (the closer to 1.0 the better ) with human judgements, while it is acknowledged to be unrelated to human assessment if the correlation is 0. Since the Spearman's rank correlation coefficients produced by different similarity methods are dependent on the human ratings for each dataset, we need to conduct statistical significance tests on two dependent (overlapping) correlations. Thus, the Steiger's Z Significance Test is used to calculate statistical significance test between the dependent correlation coefficients produced by different similarity methods, using a one-tailed hypothesis test for assessing the difference between two paired correlations. To illustrate the evaluation process, we have demonstrate the evaluation of a novel similarity method WPath, and we want to evaluate with some datasets and compare it to Lin method.\n\n\nfrom sematch.evaluation import WordSimEvaluation\nfrom sematch.semantic.similarity import WordNetSimilarity\nevaluation = WordSimEvaluation()\nprint evaluation.dataset_names()\nwns = WordNetSimilarity()\n#define similarity metrics\nlin = lambda x, y: wns.word_similarity(x, y, 'lin')\nwpath = lambda x, y: wns.word_similarity_wpath(x, y, 0.8)\n#evaluate similarity metrics which generates Spearman correlation score.\nprint evaluation.evaluate_metric('wpath', wpath, 'noun_simlex')\nprint evaluation.evaluate_metric('lin', lin, 'noun_simlex')\n#perform Steiger's Z significance Test\nprint evaluation.statistical_test('wpath', 'lin', 'noun_simlex')\n\n#define multilingual similarity metrics Spanish-Spanish \nwpath_es = lambda x, y: wns.monol_word_similarity(x, y, 'spa', 'path')\n#English-Spanish\nwpath_en_es = lambda x, y: wns.crossl_word_similarity(x, y, 'eng', 'spa', 'wpath')\n#Evaluate multilingual word similarity datasets\nprint evaluation.evaluate_metric('wpath_es', wpath_es, 'rg65_spanish')\nprint evaluation.evaluate_metric('wpath_en_es', wpath_en_es, 'rg65_EN-ES')\n\n\n\n\nCategory Classification Evaluation\n\n\nAlthough the word similarity correlation measure is the standard way to evaluate the semantic similarity metrics, it relies on human judgements over word pairs which may not have same performance in real applications. Therefore, apart from word similarity evaluation, the Sematch evaluation framework also includes a simple aspect category classification for Aspect Based Sentiment Analysis. We use the dataset from SemEval2015 and SemEval2016, sentence-level Aspect-based Sentiment Analysis. The original dataset can be found in \nAspect Based Sentiment Analysis 15\n and \nAspect Based Sentiment Analysis 16\n.\n\n\nTo evaluate the mode, you need to first define a word similarity measurement function, and then train and evaluate the classification model.\n\n\nfrom sematch.evaluation import AspectEvaluation\nfrom sematch.application import SimClassifier, SimSVMClassifier\nfrom sematch.semantic.similarity import WordNetSimilarity\n\n# create aspect classification evaluation\nevaluation = AspectEvaluation()\n\n# load the dataset\nX, y = evaluation.load_dataset()\n\n# define word similarity function\nwns = WordNetSimilarity()\nword_sim = lambda x, y: wns.word_similarity(x, y)\n\n# Train and evaluate metrics with unsupervised classification model\nsimclassifier = SimClassifier.train(zip(X,y), word_sim)\nevaluation.evaluate(X,y, simclassifier)\n\n-----Evaluation Results-----\nmacro averge:  (0.65319812882333839, 0.7101245049198579, 0.66317566364913016, None)\nmicro average:  (0.79210167952791644, 0.79210167952791644, 0.79210167952791644, None)\nweighted average:  (0.80842645056024054, 0.79210167952791644, 0.79639496616636352, None)\naccuracy:  0.792101679528\n             precision    recall  f1-score   support\n\n    SERVICE       0.50      0.43      0.46       519\n RESTAURANT       0.81      0.66      0.73       228\n       FOOD       0.95      0.87      0.91      2256\n   LOCATION       0.26      0.67      0.37        54\n   AMBIENCE       0.60      0.70      0.65       597\n     DRINKS       0.81      0.93      0.87       752\n\navg / total       0.81      0.79      0.80      4406\n\n           |                        R      |\n           |                        E      |\n           |    A              L    S      |\n           |    M              O    T    S |\n           |    B    D         C    A    E |\n           |    I    R         A    U    R |\n           |    E    I    F    T    R    V |\n           |    N    N    O    I    A    I |\n           |    C    K    O    O    N    C |\n           |    E    S    D    N    T    E |\n-----------+-------------------------------+\n  AMBIENCE | \n223\n   .   13   43  179   61 |\n    DRINKS |   15 \n151\n  54    4    1    3 |\n      FOOD |   99   24\n1960\n  37   76   60 |\n  LOCATION |    2    .    3  \n36\n  13    . |\nRESTAURANT |   81   12   29   19 \n417\n  39 |\n   SERVICE |   30    .   10    2    7 \n703\n|\n-----------+-------------------------------+\n(row = reference; col = test)\n\n\n# Train and evaluate metrics with supervised classification model\nsimSVMclassifier = SimSVMClassifier.train(X, y, word_sim) #supervised classification\nevaluation.evaluate(X, y, simSVMclassifier)\n\n-----Evaluation Results-----\nmacro averge:  (0.87738328966718993, 0.80275862524008135, 0.83522698525943129, None)\nmicro average:  (0.87721289151157511, 0.87721289151157511, 0.87721289151157511, None)\nweighted average:  (0.87555902983892719, 0.87721289151157511, 0.87314386708061753, None)\naccuracy:  0.877212891512\n             precision    recall  f1-score   support\n\n    SERVICE       0.85      0.63      0.72       519\n RESTAURANT       0.92      0.85      0.89       228\n       FOOD       0.89      0.97      0.93      2256\n   LOCATION       0.91      0.74      0.82        54\n   AMBIENCE       0.75      0.71      0.73       597\n     DRINKS       0.94      0.91      0.93       752\n\navg / total       0.88      0.88      0.87      4406\n\n           |                        R      |\n           |                        E      |\n           |    A              L    S      |\n           |    M              O    T    S |\n           |    B    D         C    A    E |\n           |    I    R         A    U    R |\n           |    E    I    F    T    R    V |\n           |    N    N    O    I    A    I |\n           |    C    K    O    O    N    C |\n           |    E    S    D    N    T    E |\n-----------+-------------------------------+\n  AMBIENCE | \n328\n   4   58    .  111   18 |\n    DRINKS |    3 \n194\n  29    .    .    2 |\n      FOOD |   22    8\n2194\n   .   19   13 |\n  LOCATION |    3    .    2  \n40\n   9    . |\nRESTAURANT |   21    4  138    4 \n422\n   8 |\n   SERVICE |    9    .   56    .    . \n687\n|\n-----------+-------------------------------+\n(row = reference; col = test)", 
            "title": "Evaluation"
        }, 
        {
            "location": "/evaluation/#word-similarity-evaluation", 
            "text": "We have collected some well known word similarity datasets for evaluating semantic similarity metrics. Several python classes can be used to prepare the dataset and evaluate the metric automatically. The word similarity datasets included in Sematch are listed as below:   Rubenstein and Goodenough (RG)     Herbert Rubenstein and John B. Goodenough. 1965. Contextual correlates of synonymy. Commun. ACM 8, 10 (October 1965), 627-633. DOI=10.1145/365628.365657    Miller and Charles (MC)     Miller, George A., and Walter G. Charles. \"Contextual correlates of semantic similarity.\" Language and cognitive processes 6.1 (1991): 1-28.   Wordsim353 (WS353)     Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin, \"Placing Search in Context: The Concept Revisited\", ACM Transactions on Information Systems, 20(1):116-131, January 2002    wordsim353 similarity and relatedness (WS353Sim)     Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pasca, Aitor Soroa, A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches, In Proceedings of NAACL-HLT 2009.   SimLex-999 (SIMLEX)     SimLex-999: Evaluating Semantic Models with (Genuine) Similarity Estimation. 2014. Felix Hill, Roi Reichart and Anna Korhonen. Preprint pubslished on arXiv. arXiv:1408.3456   Multilingual Word Similarity   Camacho-Collados, Jos\u00e9, Mohammad Taher Pilehvar, and Roberto Navigli. \"A Framework for the Construction of Monolingual and Cross-lingual Word Similarity Datasets.\" ACL (2). 2015.  When developing new similarity metrics, proper evaluation is important, whereas sometimes it is tedious. Sematch helps to save such efforts by providing a evaluation framework, where similarity methods are evaluated with common word similarity datasets and can be compared with each other.   The most established methodology for evaluating performance of semantic similarity methods in word similarity dataset, is measuring the Spearman correlation between similarity scores generated by the similarity methods and scores assessed by human. Note that both Spearman's and Pearson's correlations coefficients have been commonly used in the literatures. They are equivalent if rating scores are ordered and we use Spearman correlation coefficients as default. A similarity method is acknolwedged to have better performance if it has higher correlation score (the closer to 1.0 the better ) with human judgements, while it is acknowledged to be unrelated to human assessment if the correlation is 0. Since the Spearman's rank correlation coefficients produced by different similarity methods are dependent on the human ratings for each dataset, we need to conduct statistical significance tests on two dependent (overlapping) correlations. Thus, the Steiger's Z Significance Test is used to calculate statistical significance test between the dependent correlation coefficients produced by different similarity methods, using a one-tailed hypothesis test for assessing the difference between two paired correlations. To illustrate the evaluation process, we have demonstrate the evaluation of a novel similarity method WPath, and we want to evaluate with some datasets and compare it to Lin method.  from sematch.evaluation import WordSimEvaluation\nfrom sematch.semantic.similarity import WordNetSimilarity\nevaluation = WordSimEvaluation()\nprint evaluation.dataset_names()\nwns = WordNetSimilarity()\n#define similarity metrics\nlin = lambda x, y: wns.word_similarity(x, y, 'lin')\nwpath = lambda x, y: wns.word_similarity_wpath(x, y, 0.8)\n#evaluate similarity metrics which generates Spearman correlation score.\nprint evaluation.evaluate_metric('wpath', wpath, 'noun_simlex')\nprint evaluation.evaluate_metric('lin', lin, 'noun_simlex')\n#perform Steiger's Z significance Test\nprint evaluation.statistical_test('wpath', 'lin', 'noun_simlex')\n\n#define multilingual similarity metrics Spanish-Spanish \nwpath_es = lambda x, y: wns.monol_word_similarity(x, y, 'spa', 'path')\n#English-Spanish\nwpath_en_es = lambda x, y: wns.crossl_word_similarity(x, y, 'eng', 'spa', 'wpath')\n#Evaluate multilingual word similarity datasets\nprint evaluation.evaluate_metric('wpath_es', wpath_es, 'rg65_spanish')\nprint evaluation.evaluate_metric('wpath_en_es', wpath_en_es, 'rg65_EN-ES')", 
            "title": "Word Similarity Evaluation"
        }, 
        {
            "location": "/evaluation/#category-classification-evaluation", 
            "text": "Although the word similarity correlation measure is the standard way to evaluate the semantic similarity metrics, it relies on human judgements over word pairs which may not have same performance in real applications. Therefore, apart from word similarity evaluation, the Sematch evaluation framework also includes a simple aspect category classification for Aspect Based Sentiment Analysis. We use the dataset from SemEval2015 and SemEval2016, sentence-level Aspect-based Sentiment Analysis. The original dataset can be found in  Aspect Based Sentiment Analysis 15  and  Aspect Based Sentiment Analysis 16 .  To evaluate the mode, you need to first define a word similarity measurement function, and then train and evaluate the classification model.  from sematch.evaluation import AspectEvaluation\nfrom sematch.application import SimClassifier, SimSVMClassifier\nfrom sematch.semantic.similarity import WordNetSimilarity\n\n# create aspect classification evaluation\nevaluation = AspectEvaluation()\n\n# load the dataset\nX, y = evaluation.load_dataset()\n\n# define word similarity function\nwns = WordNetSimilarity()\nword_sim = lambda x, y: wns.word_similarity(x, y)\n\n# Train and evaluate metrics with unsupervised classification model\nsimclassifier = SimClassifier.train(zip(X,y), word_sim)\nevaluation.evaluate(X,y, simclassifier)\n\n-----Evaluation Results-----\nmacro averge:  (0.65319812882333839, 0.7101245049198579, 0.66317566364913016, None)\nmicro average:  (0.79210167952791644, 0.79210167952791644, 0.79210167952791644, None)\nweighted average:  (0.80842645056024054, 0.79210167952791644, 0.79639496616636352, None)\naccuracy:  0.792101679528\n             precision    recall  f1-score   support\n\n    SERVICE       0.50      0.43      0.46       519\n RESTAURANT       0.81      0.66      0.73       228\n       FOOD       0.95      0.87      0.91      2256\n   LOCATION       0.26      0.67      0.37        54\n   AMBIENCE       0.60      0.70      0.65       597\n     DRINKS       0.81      0.93      0.87       752\n\navg / total       0.81      0.79      0.80      4406\n\n           |                        R      |\n           |                        E      |\n           |    A              L    S      |\n           |    M              O    T    S |\n           |    B    D         C    A    E |\n           |    I    R         A    U    R |\n           |    E    I    F    T    R    V |\n           |    N    N    O    I    A    I |\n           |    C    K    O    O    N    C |\n           |    E    S    D    N    T    E |\n-----------+-------------------------------+\n  AMBIENCE |  223    .   13   43  179   61 |\n    DRINKS |   15  151   54    4    1    3 |\n      FOOD |   99   24 1960   37   76   60 |\n  LOCATION |    2    .    3   36   13    . |\nRESTAURANT |   81   12   29   19  417   39 |\n   SERVICE |   30    .   10    2    7  703 |\n-----------+-------------------------------+\n(row = reference; col = test)\n\n\n# Train and evaluate metrics with supervised classification model\nsimSVMclassifier = SimSVMClassifier.train(X, y, word_sim) #supervised classification\nevaluation.evaluate(X, y, simSVMclassifier)\n\n-----Evaluation Results-----\nmacro averge:  (0.87738328966718993, 0.80275862524008135, 0.83522698525943129, None)\nmicro average:  (0.87721289151157511, 0.87721289151157511, 0.87721289151157511, None)\nweighted average:  (0.87555902983892719, 0.87721289151157511, 0.87314386708061753, None)\naccuracy:  0.877212891512\n             precision    recall  f1-score   support\n\n    SERVICE       0.85      0.63      0.72       519\n RESTAURANT       0.92      0.85      0.89       228\n       FOOD       0.89      0.97      0.93      2256\n   LOCATION       0.91      0.74      0.82        54\n   AMBIENCE       0.75      0.71      0.73       597\n     DRINKS       0.94      0.91      0.93       752\n\navg / total       0.88      0.88      0.87      4406\n\n           |                        R      |\n           |                        E      |\n           |    A              L    S      |\n           |    M              O    T    S |\n           |    B    D         C    A    E |\n           |    I    R         A    U    R |\n           |    E    I    F    T    R    V |\n           |    N    N    O    I    A    I |\n           |    C    K    O    O    N    C |\n           |    E    S    D    N    T    E |\n-----------+-------------------------------+\n  AMBIENCE |  328    4   58    .  111   18 |\n    DRINKS |    3  194   29    .    .    2 |\n      FOOD |   22    8 2194    .   19   13 |\n  LOCATION |    3    .    2   40    9    . |\nRESTAURANT |   21    4  138    4  422    8 |\n   SERVICE |    9    .   56    .    .  687 |\n-----------+-------------------------------+\n(row = reference; col = test)", 
            "title": "Category Classification Evaluation"
        }, 
        {
            "location": "/nlp/", 
            "text": "Natural Language Processing\n\n\nThe nlp.py module provides several simple natural language processing functions, including tokenization, stopwords filtering, lemmatization, part of speech tagging (POS) using NLTK. This module is mainly used for processing textual data in WordNet and DBpedia, such as WordNet synset glosses and DBpedia abstracts and categories. Apart from NLTK, we implement a rule-based word and chunk extraction module Extraction and a keyword extraction algrithm RAKE, in order to extract words, chunks, and keywords features from WordNet and DBpedia.\n\n\nKeywords Extraction\n\n\nWith entity abstracts, you can use RAKE algorithm to extract keywords from abstract.\n\n\nfrom sematch.nlp import RAKE\nfrom sematch.semantic.sparql import EntityFeatures\nupm = EntityFeatures().features('http://dbpedia.org/resource/Technical_University_of_Madrid')\nrake = RAKE()\nprint rake.extract(upm['abstract'])\n(u'madrid (spanish: universidad polit\\xe9cnica de madrid, upm)', u'spanish university, located', \nu'madrid.', u'architecture, originating mainly', u'spain,', u'el mundo,', u'time network,', \nu'fields,', u'fifty engineering schools throughout europe.',\n u'top technical university', u'technical university')\n\n\n\n\nWords and Entity Extraction\n\n\nApart from keywords, you can use Extraction class to extract nouns and proper nouns from entity abstract.\n\n\nfrom sematch.nlp import Extraction\nfrom sematch.semantic.sparql import EntityFeatures\nupm = EntityFeatures().features('http://dbpedia.org/resource/Technical_University_of_Madrid')\nextract = Extraction()\nprint extract.extract_nouns(upm['abstract'])\n[u'technical', u'university', u'madrid', u'polytechnic', u'university',\n u'madrid', u'spanish', u'universidad', u'polit\\xe9cnica', u'madrid', \n u'upm', u'university', u'madrid', u'result', u'technical', u'school',\n  u'engineer', u'architecture', u'century', u'student', u'class', u'year',\n   u'university', u'el', u'mundo', u'technical', u'university', u'madrid', \n   u'rank', u'university', u'spain', u'majority', u'engineer', u'school', \n   u'institution', u'spain', u'field', u'europe', u'upm', u'part', u'time', \n   u'network', u'group', u'engineer', u'school', u'europe']\n\nprint extract.extract_verbs(upm['abstract'])\n[u'call', u'locate', u'found', u'merge', u'originate', u'attend', u'accord', \nu'rank', u'conduct', u'rank', u'lead', u'fifty']\n\nprint extract.extract_chunks_doc(upm['abstract'])\n[u'Technical University', u'Madrid', u'Polytechnic University', \nu'Madrid', u'Universidad Polit\\xe9cnica', u'Madrid', u'University', \nu'Madrid', u'Technical Schools', u'Engineering', u'Architecture', u'El Mundo',\n u'Technical University', u'Madrid', u'Spain', u'Engineering Schools', \n u'Spain', u'Europe', u'UPM', u'TIME', u'Europe']\n\ncats = extract.category_features(upm['category'])\nprint extract.category2words(cats)\n[u'technical', u'madrid', u'university', u'public', u'forestry', \nu'college', u'establishment', u'institution', u'spain']\n\n\n\n\nWords Rank\n\n\nIn order to rank words in entity abstract according to their topical cohenrence, you can use sematch to implement a graph-based ranking algorithm using word similarity graph. First, extract nouns in entity abstract and compute their pairwise similarity using WordNetSimilarity. Then nouns and similarity scores are parsed into SimGraph to rank words based on PageRank algorithms. In the end, top-N words are aquired using PageRank socre based on word's topical centrallity. \n\n\nfrom sematch.semantic.graph import SimGraph\nfrom sematch.semantic.similarity import WordNetSimilarity\nfrom sematch.nlp import Extraction, word_process\nfrom sematch.semantic.sparql import EntityFeatures\nfrom collections import Counter\ntom = EntityFeatures().features('http://dbpedia.org/resource/Tom_Cruise')\nwords = Extraction().extract_nouns(tom['abstract'])\nwords = word_process(words)\nwns = WordNetSimilarity()\nword_graph = SimGraph(words, wns.word_similarity)\nword_scores = word_graph.page_rank()\nwords, scores =zip(*Counter(word_scores).most_common(10))\nprint words\n(u'picture', u'action', u'number', u'film', u'post', u'sport', \nu'program', u'men', u'performance', u'motion')", 
            "title": "NLP"
        }, 
        {
            "location": "/nlp/#natural-language-processing", 
            "text": "The nlp.py module provides several simple natural language processing functions, including tokenization, stopwords filtering, lemmatization, part of speech tagging (POS) using NLTK. This module is mainly used for processing textual data in WordNet and DBpedia, such as WordNet synset glosses and DBpedia abstracts and categories. Apart from NLTK, we implement a rule-based word and chunk extraction module Extraction and a keyword extraction algrithm RAKE, in order to extract words, chunks, and keywords features from WordNet and DBpedia.", 
            "title": "Natural Language Processing"
        }, 
        {
            "location": "/nlp/#keywords-extraction", 
            "text": "With entity abstracts, you can use RAKE algorithm to extract keywords from abstract.  from sematch.nlp import RAKE\nfrom sematch.semantic.sparql import EntityFeatures\nupm = EntityFeatures().features('http://dbpedia.org/resource/Technical_University_of_Madrid')\nrake = RAKE()\nprint rake.extract(upm['abstract'])\n(u'madrid (spanish: universidad polit\\xe9cnica de madrid, upm)', u'spanish university, located', \nu'madrid.', u'architecture, originating mainly', u'spain,', u'el mundo,', u'time network,', \nu'fields,', u'fifty engineering schools throughout europe.',\n u'top technical university', u'technical university')", 
            "title": "Keywords Extraction"
        }, 
        {
            "location": "/nlp/#words-and-entity-extraction", 
            "text": "Apart from keywords, you can use Extraction class to extract nouns and proper nouns from entity abstract.  from sematch.nlp import Extraction\nfrom sematch.semantic.sparql import EntityFeatures\nupm = EntityFeatures().features('http://dbpedia.org/resource/Technical_University_of_Madrid')\nextract = Extraction()\nprint extract.extract_nouns(upm['abstract'])\n[u'technical', u'university', u'madrid', u'polytechnic', u'university',\n u'madrid', u'spanish', u'universidad', u'polit\\xe9cnica', u'madrid', \n u'upm', u'university', u'madrid', u'result', u'technical', u'school',\n  u'engineer', u'architecture', u'century', u'student', u'class', u'year',\n   u'university', u'el', u'mundo', u'technical', u'university', u'madrid', \n   u'rank', u'university', u'spain', u'majority', u'engineer', u'school', \n   u'institution', u'spain', u'field', u'europe', u'upm', u'part', u'time', \n   u'network', u'group', u'engineer', u'school', u'europe']\n\nprint extract.extract_verbs(upm['abstract'])\n[u'call', u'locate', u'found', u'merge', u'originate', u'attend', u'accord', \nu'rank', u'conduct', u'rank', u'lead', u'fifty']\n\nprint extract.extract_chunks_doc(upm['abstract'])\n[u'Technical University', u'Madrid', u'Polytechnic University', \nu'Madrid', u'Universidad Polit\\xe9cnica', u'Madrid', u'University', \nu'Madrid', u'Technical Schools', u'Engineering', u'Architecture', u'El Mundo',\n u'Technical University', u'Madrid', u'Spain', u'Engineering Schools', \n u'Spain', u'Europe', u'UPM', u'TIME', u'Europe']\n\ncats = extract.category_features(upm['category'])\nprint extract.category2words(cats)\n[u'technical', u'madrid', u'university', u'public', u'forestry', \nu'college', u'establishment', u'institution', u'spain']", 
            "title": "Words and Entity Extraction"
        }, 
        {
            "location": "/nlp/#words-rank", 
            "text": "In order to rank words in entity abstract according to their topical cohenrence, you can use sematch to implement a graph-based ranking algorithm using word similarity graph. First, extract nouns in entity abstract and compute their pairwise similarity using WordNetSimilarity. Then nouns and similarity scores are parsed into SimGraph to rank words based on PageRank algorithms. In the end, top-N words are aquired using PageRank socre based on word's topical centrallity.   from sematch.semantic.graph import SimGraph\nfrom sematch.semantic.similarity import WordNetSimilarity\nfrom sematch.nlp import Extraction, word_process\nfrom sematch.semantic.sparql import EntityFeatures\nfrom collections import Counter\ntom = EntityFeatures().features('http://dbpedia.org/resource/Tom_Cruise')\nwords = Extraction().extract_nouns(tom['abstract'])\nwords = word_process(words)\nwns = WordNetSimilarity()\nword_graph = SimGraph(words, wns.word_similarity)\nword_scores = word_graph.page_rank()\nwords, scores =zip(*Counter(word_scores).most_common(10))\nprint words\n(u'picture', u'action', u'number', u'film', u'post', u'sport', \nu'program', u'men', u'performance', u'motion')", 
            "title": "Words Rank"
        }, 
        {
            "location": "/application/", 
            "text": "Semantic Search\n\n\nSematch offers semantic search in terms of searching entities (e.g. \nhttp://dbpedia.org/resource/Tom Cruise\n)  and concepts (e.g.\nhttp://dbpedia.org/ontology/Actor\n), which are implemented in \nMatcher\n class.\n\n\nSearch LOD concept links\n\n\nUsers may need to search LOD concept links through natural language words. Sematch offers this search with Multilingual support.  An input word will be mapped to WordNet synsets, and then the mapped synsets having LOD links are retrieved. We provide 68423 synset-yago mappings by processing YAGO data. Note that we only consider common nouns. You can use different languages such as English words, car, university, singer; Spanish words, coche, universidad, cantante; Chinese words, \u6c7d\u8f66, \u5927\u5b66, \u6b4c\u624b, to search for links. \n\n\nfrom sematch.application import Matcher\nmatcher = Matcher()\n#use English word to search LOD ontology class links \nprint matcher.type_links('actor')\n#use Spanish word to search LOD ontology class links \nprint matcher.type_links('coche', lang='spa')\n#use Chinese word to search LOD ontology class links \nprint matcher.type_links('\u732b', lang='cmn')\n\n\n\n\nSematch supports Taxonomy based concept expansion. Users need to enable the expansion with the following code example.\n\n\nfrom sematch.application import Matcher\nmatcher = Matcher(expansion=True)\nprint matcher.type_links('singer')\n\n\n\n\nWith concept expansion, Sematch will perform depth-based tree search in WordNet taxonomy to find all the sub-concepts. For example,  singer's sub-concept baritone will be returned as well given query singer. Note that the results may be different with different languages because the lemma to synset mapping are different for different languages.\n\n\nSearch entities using concepts\n\n\nHaving LOD concept links, you can search entities belonging to a specific type from DBpedia using SPARQL queries through rdf:type.  We use SPARQL templates to construct such queries automatically and exectue the SPARQL queries in \nDBpedia Sparql Endpoint\n remotely. With the following code examples, entities belonging to scientist type will be retrieved, including their name, abstracts, and DBpedia links. With different language queries, Sematch will return entity's textual label and abstract in corresponding language. Sematch first use concept search to find concept links, and then those concept links are used to construct SPARQL queries. An example of automatically generated SPARQL query is shown.\n\n\nmatcher.match_type('scientist')\nmatcher.match_type('cient\u00edfico', 'spa')\nmatcher.match_type('\u79d1\u5b66\u5bb6', 'cmn')\n\n\n\n\nSELECT DISTINCT ?s, ?label, ?abstract WHERE {\n    {  \n    ?s \nhttp://www.w3.org/1999/02/22-rdf-syntax-ns#type\n \nhttp://dbpedia.org/class/yago/NuclearPhysicist110364643\n . }\n UNION {  \n    ?s \nhttp://www.w3.org/1999/02/22-rdf-syntax-ns#type\n \nhttp://dbpedia.org/class/yago/Econometrician110043491\n . }\n UNION {  \n    ?s \nhttp://www.w3.org/1999/02/22-rdf-syntax-ns#type\n \nhttp://dbpedia.org/class/yago/Sociologist110620758\n . }\n UNION {  \n    ?s \nhttp://www.w3.org/1999/02/22-rdf-syntax-ns#type\n \nhttp://dbpedia.org/class/yago/Archeologist109804806\n . }\n UNION {  \n    ?s \nhttp://www.w3.org/1999/02/22-rdf-syntax-ns#type\n \nhttp://dbpedia.org/class/yago/Neurolinguist110354053\n . } \n    ?s \nhttp://www.w3.org/1999/02/22-rdf-syntax-ns#type\n \nhttp://www.w3.org/2002/07/owl#Thing\n . \n    ?s \nhttp://www.w3.org/2000/01/rdf-schema#label\n ?label . \n    FILTER( lang(?label) = \nen\n) . \n    ?s \nhttp://dbpedia.org/ontology/abstract\n ?abstract . \n    FILTER( lang(?abstract) = \nen\n) .\n} LIMIT 5000\n\n\n\n\nSince DBpedia endpoint has query size limitation, Sematch will decompose bigger queries into smaller ones and extecute them separately. Then results are combined when all the SPARQL queries are exectuted. Thus, the query execution may be slow if a query can be mapped to multiple concepts or query expansion is enabled. \n\n\nSearch entities using concepts and entity restriction\n\n\nIt is common that users may only want to search for a smaller group of entities having a specific type and some restrctions such as location, person, event to name a few. For example,  searching for someone's song or movie (e.g \nmovies of Tom Cruise\n.) or searching for \nscientists in Spain\n. Such queries are composed by a concept and an entity, representing information needs of entities having type of the concept and having some relations with the entity(e.g, \nmovies\n that have some relation with entity \nTom Cruise\n).  Usually, concepts correspond to meaningful common nouns, while entities correspond to specific proper nouns (e.g. university is common noun and Madrid is proper noun, query university of Madrid refers to all the universities located in Madrid). Sematch uses template to formulate SPARQL queries for such query patterns. The common nouns and proper nouns in the query are linked to concept and entity respectively. Then concept and entity links are used to construct the SPARQL query. Finally, the execution results give you a list of entities having type (rdf:type) of common noun, which have some semantic relation with proper noun. In addition, if query expansion is enabled, concepts will be expanded based on WordNet taxonomy in order to match more related entities. You can check out the technical details in the following publication.   \n\n\n\n\nGanggao Zhu, and Carlos Angel Iglesias. \"Sematch: Semantic Entity Search from Knowledge Graph.\" SumPre-HSWI@ ESWC. 2015.\n\n\n\n\nThe common noun to YAGO concept mapping is based on \ntype_link\n function as shown above, while the proper noun to DBpedia instance mapping is currently based on exact string matching using SPARQL queries. So it is required to make sure that the spell and string of the instance names are correct.\n\n\nfrom sematch.sparql import NameSPARQL\n#a simple linker to match proper noun through labels and names.\nname_linker = NameSPARQL()\nname_linker.name2entities('China')#'http://dbpedia.org/resource/China'\nname_linker.name2entities('Spain')#'http://dbpedia.org/resource/Spain'\nname_linker.name2entities('Tom Cruise')#'http://dbpedia.org/resource/Tom_Cruise'\n\n\n\n\nFor example, to map \nTom Cruise\n to proper instance, the following SPARQL query is executed. \n\n\nSELECT DISTINCT ?s WHERE {\n    {  \n    ?s \nhttp://www.w3.org/2000/01/rdf-schema#label\n \nTom Cruise\n@en . }\n UNION {  \n    ?s \nhttp://www.w3.org/2000/01/rdf-schema#label\n \nTom Cruise\n@en . }\n UNION {  \n    ?x \nhttp://www.w3.org/2000/01/rdf-schema#label\n \nTom Cruise\n@en . \n    ?x \nhttp://dbpedia.org/ontology/wikiPageRedirects\n ?s . }\n UNION {  \n    ?s \nhttp://dbpedia.org/ontology/demonym\n \nTom Cruise\n@en . }\n UNION {  \n    ?s \nhttp://dbpedia.org/property/acronym\n \nTom Cruise\n@en . } \n    ?s \nhttp://www.w3.org/1999/02/22-rdf-syntax-ns#type\n \nhttp://www.w3.org/2002/07/owl#Thing\n .\n} LIMIT 5000\nSELECT DISTINCT ?s WHERE {\n\n    \nhttp://dbpedia.org/resource/Tom_Cruise\n \nhttp://dbpedia.org/ontology/wikiPageRedirects\n ?s .\n} LIMIT 5000\n\n\n\n\nThe following examples are illustrated to search entities with concept and restriction. We do not provide mutilingual version of this function because of the query processing problem. User can implement the multilingual version with proper segmentation and proper noun or entity extraction. Currently, we use rule based noun extraction and named phrase extraction for English text.                                      \n\n\nprint matcher.match_entity_type('university in Madrid')\nprint matcher.match_entity_type('movies with Tom Cruise')\n\n\n\n\nFor example, to match \nmovies with Tom Cruise\n, the SPARQL query is shown. \n\n\nSELECT DISTINCT ?s, ?label, ?abstract WHERE {\n    {  \n    ?s \nhttp://www.w3.org/1999/02/22-rdf-syntax-ns#type\n \nhttp://dbpedia.org/class/yago/Movie106613686\n . }\n UNION {  \n    ?s \nhttp://www.w3.org/1999/02/22-rdf-syntax-ns#type\n \nhttp://dbpedia.org/ontology/Film\n . } \n    \nhttp://dbpedia.org/resource/Tom_Cruise\n ?p ?s . \n    ?s \nhttp://www.w3.org/2000/01/rdf-schema#label\n ?label . \n    FILTER( lang(?label) = \nen\n) . \n    ?s \nhttp://dbpedia.org/ontology/abstract\n ?abstract . \n    FILTER( lang(?abstract) = \nen\n) .\n} LIMIT 5000\n\n\n\n\nSimilarity-Based Classification\n\n\nWe have implemented two similarity based classification model (unsupervised \nSimClassifier\n and supvervised \nSimSVMClassifier\n) in order to evaluate similarity metrics and provide similarity based text classification. \n\n\nThe idea of similarity based classification is to extract most N frequent words associated with a class label so that those words are used as features to represet the label. Then, with given target words, by comparing the semantic similarity between target words and feature words, the correct label can be determined. The unsupervised \nSimClassifier\n chooses the corect label with the highest similarity score. The supervised \nSimSVMClassifier\n employs a Support Vector Machine (SVM) with a linear kernel and the trained SVM classifier is used to predict the correct label. Within similarity-based classification, the effectiveness of different knowledge-based semantic similarity metrics can be evaluated in applications apart from word similarities.\n\n\nfrom sematch.evaluation import AspectEvaluation\nfrom sematch.application import SimClassifier, SimSVMClassifier\nfrom sematch.semantic.similarity import WordNetSimilarity\nevaluation = AspectEvaluation()\nX, y = evaluation.load_dataset()\nwns = WordNetSimilarity()\nsim_metric = wns.word_similarity\nsimclassifier = SimClassifier.train(zip(X,y), sim_metric)\nevaluation.evaluate(X,y, simclassifier)\nsimSVMclassifier = SimSVMClassifier.train(X, y, sim_metric)\nevaluation.evaluate(X, y, simSVMclassifier)\n\n\n\n\nWe use the sentence-level Aspect-based Sentiment Analysis for illustration. In this task, aspect target words are associated to a list of aspect categories. We use the restaurant domain, which contains category classes of SERVICE, FOOD, DRINKS, AMBIENCE, etc. The aspect category classification task consists in assigning aspect category to opinion target words. This task challenges semantic relatedness methods, especially for corpus-based methods. For instance, in restaurant review corpora, those target words such as fish and wine would appear in same surrounding contexts (e.g., \u201cthe fish is delicious and the wine is great\u201d). Since corpus-based methods are based on calculating co-occurrences of terms in a corpus, they can hardly discriminate terms from different categories that are frequently collocated (e.g., fish and wine). In such scenario, knowledge-based methods are useful to include the structural knowledge from domain taxonomy. Semantic similarity methods can be used to measure the taxonomical similarity between target words and aspect category in order to classify the target words into correct aspect category. We processed the dataset and got 4406 tuples containing target words and their categories. For example, (\"wine list\", \"DRINKS\") and (\"spicy tuna roll\", \"FOOD\"). For more details, reader can check the following articles.\n\n\n\n\n\n\nZhu, Ganggao, and Carlos A. Iglesias. \n\"Computing Semantic Similarity of Concepts in Knowledge Graphs.\"\n IEEE Transactions on Knowledge and Data Engineering 29.1 (2017): 72-85.\n\n\n\n\n\n\nOscar Araque, Ganggao Zhu, Manuel Garcia-Amado and Carlos A. Iglesias \nMining the Opinionated Web: Classification and Detection of Aspect Contexts for Aspect Based Sentiment Analysis\n,  ICDM sentire, 2016.", 
            "title": "Application"
        }, 
        {
            "location": "/application/#semantic-search", 
            "text": "Sematch offers semantic search in terms of searching entities (e.g.  http://dbpedia.org/resource/Tom Cruise )  and concepts (e.g. http://dbpedia.org/ontology/Actor ), which are implemented in  Matcher  class.", 
            "title": "Semantic Search"
        }, 
        {
            "location": "/application/#search-lod-concept-links", 
            "text": "Users may need to search LOD concept links through natural language words. Sematch offers this search with Multilingual support.  An input word will be mapped to WordNet synsets, and then the mapped synsets having LOD links are retrieved. We provide 68423 synset-yago mappings by processing YAGO data. Note that we only consider common nouns. You can use different languages such as English words, car, university, singer; Spanish words, coche, universidad, cantante; Chinese words, \u6c7d\u8f66, \u5927\u5b66, \u6b4c\u624b, to search for links.   from sematch.application import Matcher\nmatcher = Matcher()\n#use English word to search LOD ontology class links \nprint matcher.type_links('actor')\n#use Spanish word to search LOD ontology class links \nprint matcher.type_links('coche', lang='spa')\n#use Chinese word to search LOD ontology class links \nprint matcher.type_links('\u732b', lang='cmn')  Sematch supports Taxonomy based concept expansion. Users need to enable the expansion with the following code example.  from sematch.application import Matcher\nmatcher = Matcher(expansion=True)\nprint matcher.type_links('singer')  With concept expansion, Sematch will perform depth-based tree search in WordNet taxonomy to find all the sub-concepts. For example,  singer's sub-concept baritone will be returned as well given query singer. Note that the results may be different with different languages because the lemma to synset mapping are different for different languages.", 
            "title": "Search LOD concept links"
        }, 
        {
            "location": "/application/#search-entities-using-concepts", 
            "text": "Having LOD concept links, you can search entities belonging to a specific type from DBpedia using SPARQL queries through rdf:type.  We use SPARQL templates to construct such queries automatically and exectue the SPARQL queries in  DBpedia Sparql Endpoint  remotely. With the following code examples, entities belonging to scientist type will be retrieved, including their name, abstracts, and DBpedia links. With different language queries, Sematch will return entity's textual label and abstract in corresponding language. Sematch first use concept search to find concept links, and then those concept links are used to construct SPARQL queries. An example of automatically generated SPARQL query is shown.  matcher.match_type('scientist')\nmatcher.match_type('cient\u00edfico', 'spa')\nmatcher.match_type('\u79d1\u5b66\u5bb6', 'cmn')  SELECT DISTINCT ?s, ?label, ?abstract WHERE {\n    {  \n    ?s  http://www.w3.org/1999/02/22-rdf-syntax-ns#type   http://dbpedia.org/class/yago/NuclearPhysicist110364643  . }\n UNION {  \n    ?s  http://www.w3.org/1999/02/22-rdf-syntax-ns#type   http://dbpedia.org/class/yago/Econometrician110043491  . }\n UNION {  \n    ?s  http://www.w3.org/1999/02/22-rdf-syntax-ns#type   http://dbpedia.org/class/yago/Sociologist110620758  . }\n UNION {  \n    ?s  http://www.w3.org/1999/02/22-rdf-syntax-ns#type   http://dbpedia.org/class/yago/Archeologist109804806  . }\n UNION {  \n    ?s  http://www.w3.org/1999/02/22-rdf-syntax-ns#type   http://dbpedia.org/class/yago/Neurolinguist110354053  . } \n    ?s  http://www.w3.org/1999/02/22-rdf-syntax-ns#type   http://www.w3.org/2002/07/owl#Thing  . \n    ?s  http://www.w3.org/2000/01/rdf-schema#label  ?label . \n    FILTER( lang(?label) =  en ) . \n    ?s  http://dbpedia.org/ontology/abstract  ?abstract . \n    FILTER( lang(?abstract) =  en ) .\n} LIMIT 5000  Since DBpedia endpoint has query size limitation, Sematch will decompose bigger queries into smaller ones and extecute them separately. Then results are combined when all the SPARQL queries are exectuted. Thus, the query execution may be slow if a query can be mapped to multiple concepts or query expansion is enabled.", 
            "title": "Search entities using concepts"
        }, 
        {
            "location": "/application/#search-entities-using-concepts-and-entity-restriction", 
            "text": "It is common that users may only want to search for a smaller group of entities having a specific type and some restrctions such as location, person, event to name a few. For example,  searching for someone's song or movie (e.g  movies of Tom Cruise .) or searching for  scientists in Spain . Such queries are composed by a concept and an entity, representing information needs of entities having type of the concept and having some relations with the entity(e.g,  movies  that have some relation with entity  Tom Cruise ).  Usually, concepts correspond to meaningful common nouns, while entities correspond to specific proper nouns (e.g. university is common noun and Madrid is proper noun, query university of Madrid refers to all the universities located in Madrid). Sematch uses template to formulate SPARQL queries for such query patterns. The common nouns and proper nouns in the query are linked to concept and entity respectively. Then concept and entity links are used to construct the SPARQL query. Finally, the execution results give you a list of entities having type (rdf:type) of common noun, which have some semantic relation with proper noun. In addition, if query expansion is enabled, concepts will be expanded based on WordNet taxonomy in order to match more related entities. You can check out the technical details in the following publication.      Ganggao Zhu, and Carlos Angel Iglesias. \"Sematch: Semantic Entity Search from Knowledge Graph.\" SumPre-HSWI@ ESWC. 2015.   The common noun to YAGO concept mapping is based on  type_link  function as shown above, while the proper noun to DBpedia instance mapping is currently based on exact string matching using SPARQL queries. So it is required to make sure that the spell and string of the instance names are correct.  from sematch.sparql import NameSPARQL\n#a simple linker to match proper noun through labels and names.\nname_linker = NameSPARQL()\nname_linker.name2entities('China')#'http://dbpedia.org/resource/China'\nname_linker.name2entities('Spain')#'http://dbpedia.org/resource/Spain'\nname_linker.name2entities('Tom Cruise')#'http://dbpedia.org/resource/Tom_Cruise'  For example, to map  Tom Cruise  to proper instance, the following SPARQL query is executed.   SELECT DISTINCT ?s WHERE {\n    {  \n    ?s  http://www.w3.org/2000/01/rdf-schema#label   Tom Cruise @en . }\n UNION {  \n    ?s  http://www.w3.org/2000/01/rdf-schema#label   Tom Cruise @en . }\n UNION {  \n    ?x  http://www.w3.org/2000/01/rdf-schema#label   Tom Cruise @en . \n    ?x  http://dbpedia.org/ontology/wikiPageRedirects  ?s . }\n UNION {  \n    ?s  http://dbpedia.org/ontology/demonym   Tom Cruise @en . }\n UNION {  \n    ?s  http://dbpedia.org/property/acronym   Tom Cruise @en . } \n    ?s  http://www.w3.org/1999/02/22-rdf-syntax-ns#type   http://www.w3.org/2002/07/owl#Thing  .\n} LIMIT 5000\nSELECT DISTINCT ?s WHERE {\n\n     http://dbpedia.org/resource/Tom_Cruise   http://dbpedia.org/ontology/wikiPageRedirects  ?s .\n} LIMIT 5000  The following examples are illustrated to search entities with concept and restriction. We do not provide mutilingual version of this function because of the query processing problem. User can implement the multilingual version with proper segmentation and proper noun or entity extraction. Currently, we use rule based noun extraction and named phrase extraction for English text.                                        print matcher.match_entity_type('university in Madrid')\nprint matcher.match_entity_type('movies with Tom Cruise')  For example, to match  movies with Tom Cruise , the SPARQL query is shown.   SELECT DISTINCT ?s, ?label, ?abstract WHERE {\n    {  \n    ?s  http://www.w3.org/1999/02/22-rdf-syntax-ns#type   http://dbpedia.org/class/yago/Movie106613686  . }\n UNION {  \n    ?s  http://www.w3.org/1999/02/22-rdf-syntax-ns#type   http://dbpedia.org/ontology/Film  . } \n     http://dbpedia.org/resource/Tom_Cruise  ?p ?s . \n    ?s  http://www.w3.org/2000/01/rdf-schema#label  ?label . \n    FILTER( lang(?label) =  en ) . \n    ?s  http://dbpedia.org/ontology/abstract  ?abstract . \n    FILTER( lang(?abstract) =  en ) .\n} LIMIT 5000", 
            "title": "Search entities using concepts and entity restriction"
        }, 
        {
            "location": "/application/#similarity-based-classification", 
            "text": "We have implemented two similarity based classification model (unsupervised  SimClassifier  and supvervised  SimSVMClassifier ) in order to evaluate similarity metrics and provide similarity based text classification.   The idea of similarity based classification is to extract most N frequent words associated with a class label so that those words are used as features to represet the label. Then, with given target words, by comparing the semantic similarity between target words and feature words, the correct label can be determined. The unsupervised  SimClassifier  chooses the corect label with the highest similarity score. The supervised  SimSVMClassifier  employs a Support Vector Machine (SVM) with a linear kernel and the trained SVM classifier is used to predict the correct label. Within similarity-based classification, the effectiveness of different knowledge-based semantic similarity metrics can be evaluated in applications apart from word similarities.  from sematch.evaluation import AspectEvaluation\nfrom sematch.application import SimClassifier, SimSVMClassifier\nfrom sematch.semantic.similarity import WordNetSimilarity\nevaluation = AspectEvaluation()\nX, y = evaluation.load_dataset()\nwns = WordNetSimilarity()\nsim_metric = wns.word_similarity\nsimclassifier = SimClassifier.train(zip(X,y), sim_metric)\nevaluation.evaluate(X,y, simclassifier)\nsimSVMclassifier = SimSVMClassifier.train(X, y, sim_metric)\nevaluation.evaluate(X, y, simSVMclassifier)  We use the sentence-level Aspect-based Sentiment Analysis for illustration. In this task, aspect target words are associated to a list of aspect categories. We use the restaurant domain, which contains category classes of SERVICE, FOOD, DRINKS, AMBIENCE, etc. The aspect category classification task consists in assigning aspect category to opinion target words. This task challenges semantic relatedness methods, especially for corpus-based methods. For instance, in restaurant review corpora, those target words such as fish and wine would appear in same surrounding contexts (e.g., \u201cthe fish is delicious and the wine is great\u201d). Since corpus-based methods are based on calculating co-occurrences of terms in a corpus, they can hardly discriminate terms from different categories that are frequently collocated (e.g., fish and wine). In such scenario, knowledge-based methods are useful to include the structural knowledge from domain taxonomy. Semantic similarity methods can be used to measure the taxonomical similarity between target words and aspect category in order to classify the target words into correct aspect category. We processed the dataset and got 4406 tuples containing target words and their categories. For example, (\"wine list\", \"DRINKS\") and (\"spicy tuna roll\", \"FOOD\"). For more details, reader can check the following articles.    Zhu, Ganggao, and Carlos A. Iglesias.  \"Computing Semantic Similarity of Concepts in Knowledge Graphs.\"  IEEE Transactions on Knowledge and Data Engineering 29.1 (2017): 72-85.    Oscar Araque, Ganggao Zhu, Manuel Garcia-Amado and Carlos A. Iglesias  Mining the Opinionated Web: Classification and Detection of Aspect Contexts for Aspect Based Sentiment Analysis ,  ICDM sentire, 2016.", 
            "title": "Similarity-Based Classification"
        }, 
        {
            "location": "/api/", 
            "text": "Similarity API\n\n\nSome common interfaces for semantic information are provided so that novel similarity metrics are easy to implement with Sematch. The main semantic resources include:\n\n\n\n\nDepth\n is the path length between a root and a specific concept in taxonomy.\n\n\nShortest Path Length\n is the shortest path length between two concepts in taxonomy.\n\n\nLeast Common Subsumer\n is the common parent of two concepts which has maximun depth.\n\n\nCorpus-based IC\n is the information content computed from sense-annotated corpora such as SemCor or Brown Corpus\n\n\nGraph-based IC\n is the information content computed from Knowledge graph\n\n\n\n\nThe word similarity interface is shown below.\n\n\n    def word_similarity(self, w1, w2, name='wpath'):\n        \n Return similarity score between two words based on WordNet.\n\n        :param w1: first word to be compared which should be contained in WordNet\n        :param w2: second word to be compared which should be contained in WordNet\n        :param name: the name of knowledge-based semantic similarity metrics\n        :return: numerical score indicating degree of similarity between two words. The\n        minimum score is 0. If one of the input words is not contained in WordNet, 0 is given. The up bound of\n        the similarity score depends on the similarity metric you use. Bigger similarity values indicate higher\n        similarity between two words.\n        :rtype : Float\n        \n\n        s1 = self.word2synset(w1)\n        s2 = self.word2synset(w2)\n        sim_metric = lambda x, y: self.similarity(x, y, name)\n        return self.max_synset_similarity(s1, s2, sim_metric)\n\n\n\n\nThe word similarity interface is used for computing similarity between words that are contained in KG. As words are ambiguous, they are first mapped to corresponding concepts in KG. Then word similarity is calculated based on maximum semantic similarity between concepts. For those words having no corresponding concepts (out of vocabulary), 0 similarity is returned. User should use word2synset or word2concept function to check if a given word can be mapped to concepts in KG.\n\n\nwns = WordNetSimilarity()\nprint wns.word2synset('potato')\n[Synset('potato.n.01'), Synset('potato.n.02')]\nprint wns.word2synset('potatoz')\n[]\n\n\n\n\nTo design a new semantic similarity metric in WordNet, you can take the implementation of WPath method as example.\n\n\ndef wpath(self, c1, c2, k=0.8):\n    lcs = self.least_common_subsumer(c1,c2)\n    path = c1.shortest_path_distance(c2)\n    weight = k ** self.synset_ic(lcs)\n    return 1.0 / (1 + path*weight)\n\n\n\n\nTo design a new semantic similarity metric for YAGO concepts, you can not only use corpus based IC \nsynset_ic\n, buth also use \nconcept_ic\n which computes graph-based IC.\n\n\ndef wpath_graph(self, c1, c2, k=0.9):\n    lcs = self.least_common_subsumer(c1, c2)\n    path = c1.shortest_path_distance(c2)\n    yago_lcs = self.synset2yago(lcs)\n    weight = k ** self._graph_ic.concept_ic(yago_lcs)\n    return 1.0 / (1 + path*weight)\n\n\n\n\nTo add new semantic similarity metric in common Taxonomy class, you use the similar interface as previous two examples.\n\n\ndef wpath(self, c1, c2, k=0.8):\n    lcs = self.least_common_subsumer(c1, c2)\n    path = self.shortest_path_length(c1, c2) - 1\n    weight = k ** self.concept_ic(lcs)\n    return 1.0 / (1 + path * weight)\n\n\n\n\nThe common Taxonomy and ConceptSimilarity class are designed for ontology classes of KGs, thus it only contains graph-based IC. The corpus-based IC computation requires concept-annotated corpus so we only provide YAGO concept with such support since YAGO concepts are built based on WordNet and corresponding synsets have existing sense-annotated corpora. In concequence, to compute YAGO concept similarity, user can use both kind of IC resources.\n\n\nTaxonomy API\n\n\nWithin the \nTaxonomy\n class, \nConceptSimilarity\n class and \nGraphIC\n class, user can parse any concept taxonomies where \nconcepts are represented hiearchically such as Wikipedia categories, \nOpen Diretory Project\n, \nMedical Subject Headings\n, \nACM Term ClassificationIn\n and many others. For those ontology classes with the support of Knowledge Graph, \nGraphIC\n can be used to compute graph-based information content. Otherwise, user needs to implement similar class of \nGraphIC\n based on specific data management system or just use those metrics only dependent on path length, depth, least common subsumer, such as path, wup, li, and wpath (set k parameter as 1).  We provide an example of using these general classes for computing semantic similarity.\n\n\nYou need to first implement \nDataTransform\n class to convert concept taxonomy data into nodes, labels, and edges. Nodes is a set of concept links. Labels is corresponding labels of nodes. Edges is list of (super-concept, sub-concept) tuples. Note that in edges those node are represented by their list index. \n\n\nfrom sematch.semantic.ontology import DBpedia\n\nclass DBpediaDataTransform(DataTransform):\n\n    def __init__(self):\n        self._ontology = DBpedia()\n\n    def transform(self):\n        nodes =  map(lambda x:x.toPython(), self._ontology.classes)\n        node_id = {n:i for i,n in enumerate(nodes)}\n        labels = [self._ontology.token(value) for i,value in enumerate(self._ontology.classes)]\n        edges = []\n        for i, node in enumerate(nodes):\n            children = self._ontology.subClass(node)\n            children = [child for child in children if child in nodes]\n            children_ids = map(lambda x:node_id[x], children)\n            for child_id in children_ids:\n                edges.append((i, child_id))\n        return nodes, labels, edges", 
            "title": "API"
        }, 
        {
            "location": "/api/#similarity-api", 
            "text": "Some common interfaces for semantic information are provided so that novel similarity metrics are easy to implement with Sematch. The main semantic resources include:   Depth  is the path length between a root and a specific concept in taxonomy.  Shortest Path Length  is the shortest path length between two concepts in taxonomy.  Least Common Subsumer  is the common parent of two concepts which has maximun depth.  Corpus-based IC  is the information content computed from sense-annotated corpora such as SemCor or Brown Corpus  Graph-based IC  is the information content computed from Knowledge graph   The word similarity interface is shown below.      def word_similarity(self, w1, w2, name='wpath'):\n          Return similarity score between two words based on WordNet.\n\n        :param w1: first word to be compared which should be contained in WordNet\n        :param w2: second word to be compared which should be contained in WordNet\n        :param name: the name of knowledge-based semantic similarity metrics\n        :return: numerical score indicating degree of similarity between two words. The\n        minimum score is 0. If one of the input words is not contained in WordNet, 0 is given. The up bound of\n        the similarity score depends on the similarity metric you use. Bigger similarity values indicate higher\n        similarity between two words.\n        :rtype : Float\n         \n        s1 = self.word2synset(w1)\n        s2 = self.word2synset(w2)\n        sim_metric = lambda x, y: self.similarity(x, y, name)\n        return self.max_synset_similarity(s1, s2, sim_metric)  The word similarity interface is used for computing similarity between words that are contained in KG. As words are ambiguous, they are first mapped to corresponding concepts in KG. Then word similarity is calculated based on maximum semantic similarity between concepts. For those words having no corresponding concepts (out of vocabulary), 0 similarity is returned. User should use word2synset or word2concept function to check if a given word can be mapped to concepts in KG.  wns = WordNetSimilarity()\nprint wns.word2synset('potato')\n[Synset('potato.n.01'), Synset('potato.n.02')]\nprint wns.word2synset('potatoz')\n[]  To design a new semantic similarity metric in WordNet, you can take the implementation of WPath method as example.  def wpath(self, c1, c2, k=0.8):\n    lcs = self.least_common_subsumer(c1,c2)\n    path = c1.shortest_path_distance(c2)\n    weight = k ** self.synset_ic(lcs)\n    return 1.0 / (1 + path*weight)  To design a new semantic similarity metric for YAGO concepts, you can not only use corpus based IC  synset_ic , buth also use  concept_ic  which computes graph-based IC.  def wpath_graph(self, c1, c2, k=0.9):\n    lcs = self.least_common_subsumer(c1, c2)\n    path = c1.shortest_path_distance(c2)\n    yago_lcs = self.synset2yago(lcs)\n    weight = k ** self._graph_ic.concept_ic(yago_lcs)\n    return 1.0 / (1 + path*weight)  To add new semantic similarity metric in common Taxonomy class, you use the similar interface as previous two examples.  def wpath(self, c1, c2, k=0.8):\n    lcs = self.least_common_subsumer(c1, c2)\n    path = self.shortest_path_length(c1, c2) - 1\n    weight = k ** self.concept_ic(lcs)\n    return 1.0 / (1 + path * weight)  The common Taxonomy and ConceptSimilarity class are designed for ontology classes of KGs, thus it only contains graph-based IC. The corpus-based IC computation requires concept-annotated corpus so we only provide YAGO concept with such support since YAGO concepts are built based on WordNet and corresponding synsets have existing sense-annotated corpora. In concequence, to compute YAGO concept similarity, user can use both kind of IC resources.", 
            "title": "Similarity API"
        }, 
        {
            "location": "/api/#taxonomy-api", 
            "text": "Within the  Taxonomy  class,  ConceptSimilarity  class and  GraphIC  class, user can parse any concept taxonomies where \nconcepts are represented hiearchically such as Wikipedia categories,  Open Diretory Project ,  Medical Subject Headings ,  ACM Term ClassificationIn  and many others. For those ontology classes with the support of Knowledge Graph,  GraphIC  can be used to compute graph-based information content. Otherwise, user needs to implement similar class of  GraphIC  based on specific data management system or just use those metrics only dependent on path length, depth, least common subsumer, such as path, wup, li, and wpath (set k parameter as 1).  We provide an example of using these general classes for computing semantic similarity.  You need to first implement  DataTransform  class to convert concept taxonomy data into nodes, labels, and edges. Nodes is a set of concept links. Labels is corresponding labels of nodes. Edges is list of (super-concept, sub-concept) tuples. Note that in edges those node are represented by their list index.   from sematch.semantic.ontology import DBpedia\n\nclass DBpediaDataTransform(DataTransform):\n\n    def __init__(self):\n        self._ontology = DBpedia()\n\n    def transform(self):\n        nodes =  map(lambda x:x.toPython(), self._ontology.classes)\n        node_id = {n:i for i,n in enumerate(nodes)}\n        labels = [self._ontology.token(value) for i,value in enumerate(self._ontology.classes)]\n        edges = []\n        for i, node in enumerate(nodes):\n            children = self._ontology.subClass(node)\n            children = [child for child in children if child in nodes]\n            children_ids = map(lambda x:node_id[x], children)\n            for child_id in children_ids:\n                edges.append((i, child_id))\n        return nodes, labels, edges", 
            "title": "Taxonomy API"
        }
    ]
}